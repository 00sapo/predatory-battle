@InProceedings{Vinet2004,
  author    = {Vinet, Hugues},
  title     = {{The Representation Levels of Music Information}},
  booktitle = {Computer Music Modeling and Retrieval},
  year      = {2004},
  editor    = {Wiil, Uffe Kock},
  pages     = {193--209},
  address   = {Berlin, Heidelberg},
  publisher = {Springer Berlin Heidelberg},
  abstract  = {The purpose of this article is to characterize the various kinds and specificities of music representations in technical systems. It shows that an appropriate division derived from existing applications relies in four main types, which are defined as the physical, signal, symbolic and knowledge levels. This fair simple and straightforward division provides a powerful grid for analyzing all kinds of musical applications, up to the ones resulting from the most recent research advances. Moreover, it is particularly adapted to exhibiting most current scientific issues in music technology as problems of conversion between various representation levels. The effectiveness of these concepts is then illustrated through an overview of existing applications functionalities, in particular from examples of recent research performed at IRCAM.},
  doi       = {10.1007/978-3-540-39900-1_17},
  file      = {:Vinet2004.pdf:PDF},
  groups    = {MMRP'19},

}

@InProceedings{Dorfer2017,
  author        = {Dorfer, Matthias and Arzt, Andreas and Widmer, Gerhard},
  title         = {Learning Audio-sheet Music Correspondences for Score Identification and Offline Alignment},
  booktitle     = {Proceedings of the 18th International Society for Music Information Retrieval Conference, {ISMIR} 2017, Suzhou, China},
  year          = {2017},
  editor        = {Sally Jo Cunningham and Zhiyao Duan and Xiao Hu and Douglas Turnbull},
  pages         = {115--122},
  abstract      = {This work addresses the problem of matching short excerpts of audio with their respective counterparts in sheet music images. We show how to employ neural network-based cross-modality embedding spaces for solving the following two sheet music-related tasks: retrieving the correct piece of sheet music from a database when given a music audio as a search query; and aligning an audio recording of a piece with the corresponding images of sheet music. We demonstrate the feasibility of this in experiments on classical piano music by five different composers (Bach, Haydn, Mozart, Beethoven and Chopin), and additionally provide a discussion on why we expect multi-modal neural networks to be a fruitful paradigm for dealing with sheet music and audio at the same time.},
  archiveprefix = {arXiv},
  arxivid       = {1707.09887},
  bibsource     = {dblp computer science bibliography, https://dblp.org},

  eprint        = {1707.09887},
  file          = {:Dorfer2017.pdf:PDF},
  groups        = {Explorative studies},
  keywords      = {rank4},

}

@Book{Deutsch2013a,
  title     = {The Psychology of Music (third Edition)},
  publisher = {Academic Press},
  year      = {2013},
  author    = {Deutsch, Diana},
  editor    = {Diana Deutsch},
  edition   = {Third Edition},
  doi       = {10.1016/B978-0-12-381460-9.00021-3},
  file      = {:Deutsch2013a.pdf:PDF},
  groups    = {MMRP'19},

}

@Article{Oramas2018,
  author    = {Oramas, Sergio and Barbieri, Francesco and Nieto, Oriol and Serra, Xavier},
  title     = {Multimodal Deep Learning for Music Genre Classification},
  journal   = {Transactions of the International Society for Music Information Retrieval},
  year      = {2018},
  volume    = {1},
  number    = {1},
  pages     = {4--21},
  doi       = {10.5334/tismir.10},
  file      = {:Oramas2018.pdf:PDF},
  groups    = {Explorative studies},
  keywords  = {rank4},
  publisher = {Ubiquity Press, Ltd.},
}

@Book{Yang2011,
  title     = {Music Emotion Recognition},
  publisher = {CRC Press, Inc.},
  year      = {2011},
  author    = {Yang, Yi-Hsuan and Chen, Homer H.},
  address   = {Boca Raton, FL, USA},
  edition   = {1},
  abstract  = {Providing acomplete review of existing work in music emotion developed in psychology and engineering, Music Emotion Recognition explains how to account for the subjective nature of emotion perception in the development of automatic music emotion recognition (MER) systems. Among the first publications dedicated to automatic MER, it begins with a comprehensiveintroduction to the essential aspects of MERincluding background, key techniques, and applications. This ground-breaking reference examines emotion from a dimensional perspective. It defines emotions in music as points in a 2D plane in terms of two of the most fundamental emotion dimensions according to psychologistsvalence and arousal. The authors present a computational framework that generalizes emotion recognition from the categorical domain to real-valued 2D space. They also: Introduce novel emotion-based music retrieval and organization methods Describe a ranking-base emotion annotation and model training method Present methods that integrate information extracted from lyrics, chord sequence, and genre metadata for improved accuracy Consider an emotion-based music retrieval system that is particularly useful for mobile devices The book details techniques for addressing the issues related to: the ambiguity and granularity of emotion description, heavy cognitive load of emotion annotation, subjectivity of emotion perception, and the semantic gap between low-level audio signal and high-level emotion perception. Complete with more than 360 useful references, 12 example MATLAB codes, and a listing of key abbreviations and acronyms, this cutting-edge guide supplies the technical understanding and tools needed to develop your own automatic MER system based on the automatic recognition model.},
  groups    = {PhD Milan},

}

@InProceedings{Raheb2014,
  author    = {Raheb, Katerina El and Ioannidis, Yannis},
  title     = {From Dance Notation to Conceptual Models},
  booktitle = {International Dental \& Medical Journal of Advanced Research},
  year      = {2014},
  publisher = {{ACM} Press},
  doi       = {10.1145/2617995.2618000},
  groups    = {PhD Milan},
}

@Article{Mueller2010,
  author    = {Müller, Meinard and Konz, Verena and Clausen, Michael and Ewert, Sebastian and Fremerey, Christian},
  title     = {A Multimodal Way of Experiencing and Exploring Music},
  journal   = {Interdisciplinary Science Reviews},
  year      = {2010},
  volume    = {35},
  number    = {2},
  pages     = {138--153},
  month     = jun,
  doi       = {10.1179/030801810x12723585301110},
  file      = {:Mueller2010.pdf:PDF},
  groups    = {Previous reviews on similar topics},
  keywords  = {rank3},
  publisher = {Informa {UK} Limited},
}

@InProceedings{Orio2011,
  author    = {Orio, Nicola and Rizo, David and Miotto, Riccardo and Schedl, Markus and Montecchio, Nicola and Lartillot, Olivier},
  title     = {Musiclef: A Benchmark Activity in Multimodal Music Information Retrieval},
  booktitle = {Proceedings of the 12th International Society for Music Information Retrieval Conference, {ISMIR} 2011, Miami, Florida, USA, October 24-28, 2011},
  year      = {2011},
  editor    = {Anssi Klapuri and Colby Leider},
  pages     = {603--608},
  publisher = {University of Miami},
  bibsource = {dblp computer science bibliography, https://dblp.org},

  file      = {:Orio2011.pdf:PDF},
  groups    = {Resources},

}

@Book{Mueller2015,
  title     = {Fundamentals of Music Processing: Audio, Analysis, Algorithms, Applications},
  publisher = {Springer Publishing Company, Incorporated},
  year      = {2015},
  author    = {Müller, Meinard},
  edition   = {1},
  abstract  = {This textbook provides both profound technological knowledge and a comprehensive treatment of essential topics in music processing and music information retrieval. Including numerous examples, figures, and exercises, this book is suited for students, lecturers, and researchers working in audio engineering, computer science, multimedia, and musicology. The book consists of eight chapters. The first two cover foundations of music representations and the Fourier transformconcepts that are then used throughout the book. In the subsequent chapters, concrete music processing tasks serve as a starting point. Each of these chapters is organized in a similar fashion and starts with a general description of the music processing scenario at hand before integrating it into a wider context. It then discussesin a mathematically rigorous wayimportant techniques and algorithms that are generally applicable to a wide range of analysis, classification, and retrieval problems. At the same time, the techniques are directly applied to a specific music processing task. By mixing theory and practice, the books goal is to offer detailed technological insights as well as a deep understanding of music processing applications. Each chapter ends with a section that includes links to the research literature, suggestions for further reading, a list of references, and exercises. The chapters are organized in a modular fashion, thus offering lecturers and readers many ways to choose, rearrange or supplement the material. Accordingly, selected chapters or individual sections can easily be integrated into courses on general multimedia, information science, signal processing, music informatics, or the digital humanities.},
  file      = {:Mueller2015.pdf:PDF},
  groups    = {Score-informed source separation, Audio-to-score alignment},
  keywords  = {rank5},
}

@InCollection{Essid2012,
  author    = {Essid, Slim and Richard, Gaël},
  title     = {Fusion of Multimodal Information in Music Content Analysis},
  booktitle = {Multimodal Music Processing},
  publisher = {Schloss Dagstuhl - Leibniz-Zentrum fuer Informatik GmbH, Wadern/Saarbruecken, Germany},
  year      = {2012},
  doi       = {10.4230/dfu.vol3.11041.37},
  file      = {:Essid2012.pdf:PDF},
  groups    = {Previous reviews on similar topics},
  keywords  = {Computer Science, 000 Computer science, knowledge, general works, rank4},
  language  = {eng},
}

@InProceedings{Degara2010,
  author    = {Degara, N. and Pena, A. and Davies, M. E. P. and Plumbley, M. D.},
  title     = {Note Onset Detection Using Rhythmic Structure},
  booktitle = {2010 IEEE International Conference on Acoustics, Speech and Signal Processing},
  year      = {2010},
  pages     = {5526--5529},
  month     = mar,
  abstract  = {In this paper we explore the relationship between the temporal and rhythmic structure of musical audio signals. Using automatically extracted rhythmic structure we present a rhythmically-aware method to combine note onset detection techniques. Our method uses top-down knowledge of repetitions of musical events to improve detection performance by modelling the temporal distribution of onset locations. Results on a publicly available database demonstrate that using musical knowledge in this way can lead to significant improvements by reducing the number of missed and spurious detections.},
  doi       = {10.1109/ICASSP.2010.5495220},
  file      = {:Degara2010.pdf:PDF},
  groups    = {Explorative studies},
  keywords  = {acoustic signal processing;audio signal processing;music;musical acoustics;note onset detection;rhythmic structure;temporal structure;musical audio signal;musical knowledge;Event detection;Rhythm;Data mining;Fuses;Telecommunications;Databases;Music information retrieval;Phase detection;Phase frequency detector;Energy measurement;Audio;music;onset detection;rhythm, rank4},
}

@InBook{Ross2006,
  chapter   = {2},
  title     = {Information Fusion in Biometrics},
  publisher = {Springer-Verlag},
  year      = {2006},
  author    = {Ross, Arun A. and Nandakumar, Karthik and Jain, Anil K.},
  address   = {Berlin, Heidelberg},
  booktitle = {Handbook of Multibiometrics},
  groups    = {PhD Milan},
}

@Article{Ewert2014,
  author   = {Ewert, S. and Pardo, B. and Müller, M. and Plumbley, M. D.},
  title    = {Score-informed Source Separation for Musical Audio Recordings: An Overview},
  journal  = {Ieee Signal Processing Magazine},
  year     = {2014},
  volume   = {31},
  number   = {3},
  pages    = {116--124},
  month    = may,
  abstract = {In recent years, source separation has been a central research topic in music signal processing, with applications in stereo-to-surround up-mixing, remixing tools for disc jockeys or producers, instrument-wise equalizing, karaoke systems, and preprocessing in music analysis tasks. Musical sound sources, however, are often strongly correlated in time and frequency, and without additional knowledge about the sources, a decomposition of a musical recording is often infeasible. To simplify this complex task, various methods have recently been proposed that exploit the availability of a musical score. The additional instrumentation and note information provided by the score guides the separation process, leading to significant improvements in terms of separation quality and robustness. A major challenge in utilizing this rich source of information is to bridge the gap between high-level musical events specified by the score and their corresponding acoustic realizations in an audio recording. In this article, we review recent developments in score-informed source separation and discuss various strategies for integrating the prior knowledge encoded by the score.},
  doi      = {10.1109/MSP.2013.2296076},
  groups   = {Score-informed source separation},
  keywords = {audio recording;audio signal processing;music;source separation;score-informed source separation;musical audio recordings;music signal processing;stereo-to-surround up-mixing;remixing tools;disc jockeys;instrument-wise equalizing;karaoke systems;music analysis task;musical sound sources;musical recording decomposition;musical score availability;note information;separation process;separation quality;high-level musical events;acoustic realizations;Instruments;Time-frequency analysis;Music;Audio recording;Source separation;Harmonic analysis;Spectrogram},
}

@Collection{Mueller2012,
  title     = {Multimodal Music Processing},
  year      = {2012},
  bibsource = {dblp computer science bibliography, https://dblp.org},

  editor    = {Meinard M{ü}ller and Masataka Goto and Markus Schedl},
  file      = {:Mueller2012.pdf:PDF},
  groups    = {Previous reviews on similar topics},
  keywords  = {rank3},
  publisher = {Schloss Dagstuhl - Leibniz-Zentrum fuer Informatik, Germany},
  series    = {Dagstuhl Follow-Ups},

  volume    = {3},
}

@InCollection{Konz2012,
  author    = {Konz, Verena and Müller, Meinard},
  title     = {A Cross-version Approach for Harmonic Analysis of Music Recordings},
  booktitle = {Multimodal Music Processing},
  publisher = {Schloss Dagstuhl - Leibniz-Zentrum fuer Informatik, Germany},
  year      = {2012},
  editor    = {Meinard M{\"{u}}ller and Masataka Goto and Markus Schedl},
  volume    = {3},
  series    = {Dagstuhl Follow-Ups},
  pages     = {53--72},
  bibsource = {dblp computer science bibliography, https://dblp.org},

  doi       = {10.4230/DFU.Vol3.11041.53},
  file      = {:Konz2012.pdf:PDF},
  groups    = {Explorative studies},
  keywords  = {rank5},
}

@Article{Presti2018,
  author   = {Presti, Giorgio and Ludovico, Luca and Baratè, Adriano},
  title    = {Investigating Interpretative Models in Music through Multi-layer Representation Formats},
  year     = {2018},
  abstract = {Multi-layer formats are becoming increasingly important in the field of music description. Thanks to their adoption, it is possible to embed into a unique digital document different representations of music contents, multiple in number and potentially heterogeneous in media type. Moreover, these descriptions can be mutually synchronized, thus providing different views of the same information entity with a customizable level of granularity. Standard use cases of multi-layer formats for music address information structuring and support to advanced fruition. The goal of the paper is to demonstrate how suitable multi-layer formats can foster analytical activities in the field of interpretative modeling and expressiveness investigation, discussing both the pedagogical roots and the educational implications of this approach. A use case focusing on the incipit of G. Mahler’s Symphony No. 5 will be presented.},
  file     = {:Presti2018.pdf:PDF},
  groups   = {Resources},
  keywords = {rank4},
}

@Article{Kharat2015,
  author   = {Kharat, V. and Thakare, K. and Sadafale, K.},
  title    = {{A Survey on Query by Singing/Humming}},
  journal  = {International Journal of Computer Applications},
  year     = {2015},
  volume   = {111},
  number   = {14},
  pages    = {39--42},
  month    = feb,
  adsnote  = {Provided by the SAO/NASA Astrophysics Data System},

  doi      = {10.5120/19608-1484},
  file     = {:Kharat2015.pdf:PDF},
  groups   = {Query-by-humming},
  keywords = {rank2},
}

@InProceedings{Kotsifakos2012,
  author    = {Kotsifakos, Alexios and Papapetrou, Panagiotis and Hollmén, Jaakko and Gunopulos, Dimitrios and Athitsos, Vassilis},
  title     = {A Survey of Query-by-humming Similarity Methods},
  booktitle = {Proceedings of the 5th International Conference on PErvasive Technologies Related to Assistive Environments},
  year      = {2012},
  series    = {PETRA '12},
  address   = {New York, NY, USA},
  publisher = {ACM},
  acmid     = {2413104},
  articleno = {5},
  doi       = {10.1145/2413097.2413104},
  file      = {:Kotsifakos2012.pdf:PDF},
  groups    = {Query-by-humming},
  keywords  = {hidden Markov models, query-by-humming, sequence matching, rank3},
  location  = {Heraklion, Crete, Greece},
  numpages  = {4},
}

@InProceedings{Mostafa2017,
  author    = {Mostafa, Naziba and Fung, Pascale},
  title     = {A Note Based Query by Humming System Using Convolutional Neural Network},
  booktitle = {Interspeech 2017, 18th Annual Conference of the International Speech Communication Association, Stockholm, Sweden, August 20-24, 2017},
  year      = {2017},
  editor    = {Francisco Lacerda},
  pages     = {3102--3106},
  publisher = {{ISCA}},
  bibsource = {dblp computer science bibliography, https://dblp.org},

  file      = {:Mostafa2017.pdf:PDF},
  groups    = {Query-by-humming},

}

@Article{Sun2017,
  author  = {Sun, Jia-qi and Lee, Seok-Pil},
  title   = {Query by Singing/humming System Based on Deep Learning},
  journal = {International Journal of Applied Engineering Research},
  year    = {2017},
  volume  = {12},
  number  = {13},
  pages   = {3752--3756},
  groups  = {Query-by-humming},
}

@InProceedings{Lv2017,
  author    = {Lv, A. and Liu, G.},
  title     = {An Effective Design for Fast Query-by-humming System with Melody Segmentation and Feature Extraction},
  booktitle = {2017 International Conference on Computer Systems, Electronics and Control (ICCSEC)},
  year      = {2017},
  pages     = {752--757},
  month     = dec,
  abstract  = {A melody segmentation scheme based on pivotal points combined Deep Auto-Encoder (DAE) for Query-by-Humming (QBH) systems is investigated. In order to deal with personalization of human voice, especially the instability in rhythm, we adopt a joint melody segmentation and encoded features extraction method. Pitch sequence between pivotal points shows characteristic differences largely due to monolithic deviation on tones, dynamic diversification in vocal range and mutative scale of tempo. The pivotal points in melody involve extreme and some extreme midpoints which depict the global trends, simultaneously exhibit robust to humming melody. Based on feature extraction, a novel melody segmentation scheme is derived to solve the problem. Through the trained DAE from the deep learning, the deep level coding features in melody are acquired. The experiment results are carried out to demonstrate the validity of the proposed scheme.},
  doi       = {10.1109/ICCSEC.2017.8446765},
  groups    = {Query-by-humming},
  keywords  = {Feature extraction;Encoding;Robustness;Heuristic algorithms;Rhythm;Indexing;Feature extraction;Melody segmentation;Pivotal points;Deep Auto-Encoder;Query-by-Humming},
}

@InProceedings{Nagavi2017,
  author    = {Nagavi, T. C. and Bhajantri, N. U.},
  title     = {A New Approach to Query by Humming Based on Modulated Frequency Features},
  booktitle = {2017 International Conference on Wireless Communications, Signal Processing and Networking (WiSPNET)},
  year      = {2017},
  pages     = {1675--1679},
  month     = mar,
  abstract  = {In this paper, we deem to utilize the specifics provided by a modulated frequency features for Query by Humming (QBH) music retrieval system based on hum. Initially music signal is transformed to an abstract domain using Empirical Mode Decomposition (EMD). Then, we impart the dimension reduced outcome as precious source for feature extraction process using a Modulation Frequency (MF) criterion. In order to evaluate the suggested approach, experiments are conducted on a database of 1495 song fragments which are manually extracted from 1200 songs and 200 hum recordings. The proposed approach effectively retrieves the desired song based on Humming Query (HQ) and affirms the importance of EMD and MF feature space.},
  doi       = {10.1109/WiSPNET.2017.8300046},
  groups    = {Query-by-humming},
  keywords  = {audio databases;audio signal processing;feature extraction;music;query processing;suggested approach;Humming Query;modulated frequency features;hum recordings;EMD feature space;MF feature space;Modulation Frequency criterion;feature extraction process;Empirical Mode Decomposition;initially music signal;Humming music retrieval system;Feature extraction;Multiple signal classification;Frequency modulation;Rhythm;Spectrogram;Empirical mode decomposition;modulation frequency features;query by humming},
}

@InProceedings{Makarand2018,
  author    = {Makarand, Velankar and Parag, Kulkarni},
  title     = {Unified Algorithm for Melodic Music Similarity and Retrieval in Query by Humming},
  booktitle = {Intelligent Computing and Information and Communication},
  year      = {2018},
  editor    = {Bhalla, Subhash and Bhateja, Vikrant and Chandavale, Anjali A. and Hiwale, Anil S. and Satapathy, Suresh Chandra},
  pages     = {373--381},
  address   = {Singapore},
  publisher = {Springer Singapore},
  abstract  = {Query by humming (QBH) is an active research area since a decade with limited commercial success. Challenges include partial imperfect queries from users, query representation and matching, fast, and accurate generation of results. Our work focus is on query presentation and matching algorithms to reduce the effective computational time and improve accuracy. We have proposed a unified algorithm for measuring melodic music similarity in QBH. It involves two different approaches for similarity measurement. They are novel mode normalized frequency algorithm using edit distance and n-gram precomputed inverted index method. This proposed algorithm is based on the study of melody representation in the form of note string and user query variations. Queries from four non-singers with no formal training of singing are used for initial testing. The preliminary results with 60 queries for 50 songs database are encouraging for the further research.},
  file      = {:Makarand2018.pdf:PDF},
  groups    = {Query-by-humming},
}

@InProceedings{Lin2016,
  author    = {Lin, Chiao-Wei and Ding, Jian-Jiun and Hu, Che-Ming},
  title     = {Advanced Query by Humming System Using Diffused Hidden Markov Model and Tempo Based Dynamic Programming},
  booktitle = {Asia-Pacific Signal and Information Processing Association Annual Summit and Conference, {APSIPA} 2016, Jeju, South Korea, December 13-16, 2016},
  year      = {2016},
  pages     = {1--7},
  publisher = {{IEEE}},
  bibsource = {dblp computer science bibliography, https://dblp.org},

  doi       = {10.1109/APSIPA.2016.7820765},
  file      = {:Lin2016.pdf:PDF},
  groups    = {Query-by-humming},
}

@InProceedings{Balke2016,
  author    = {Balke, S. and Arifi-Müller, V. and Lamprecht, L. and Müller, M.},
  title     = {Retrieving Audio Recordings Using Musical Themes},
  booktitle = {2016 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)},
  year      = {2016},
  pages     = {281--285},
  month     = mar,
  abstract  = {In 1948, Barlow and Morgenstern released a collection of about 10,000 themes of well-known instrumental pieces from the corpus of Western Classical music [1]. These monophonic themes (usually four bars long) are often the most memorable parts of a piece of music. In this paper, we report on a systematic study considering a cross-modal retrieval scenario. Using a musical theme as a query, the objective is to identify all related music recordings from a given audio collection. By adapting well-known retrieval techniques, our main goal is to get a better understanding of the various challenges including tempo deviations, musical tunings, key transpositions, and differences in the degree of polyphony between the symbolic query and the audio recordings to be retrieved. In particular, we present an oracle fusion approach that indicates upper performance limits achievable by a combination of current retrieval techniques.},
  doi       = {10.1109/ICASSP.2016.7471681},
  file      = {:Balke2016-Balke, S. and Arifi-Müller, V. and Lamprecht, L. and Müller, M._2016-Retrieving Audio Recordings Using Musical Themes.pdf:PDF;:Balke2016.pdf:PDF},
  groups    = {Explorative studies},
  issn      = {2379-190X},
  keywords  = {audio recording;content-based retrieval;information retrieval;musical instruments;audio recordings;musical themes;Barlow;Morgenstern;instrumental pieces;Western Classical music;monophonic themes;music piece;audio collection;tempo deviations;musical tunings;key transpositions;symbolic query;retrieval techniques;Audio recording;Tuning;Music;Databases;Instruments;Standards;Dictionaries;Music Information Retrieval;Query-by-Example, rank4},
}

@InProceedings{Mueller2013,
  author    = {Müller, M. and Prätzlich, T. and Bohl, B. and Veit, J.},
  title     = {Freischutz Digital: A Multimodal Scenario for Informed Music Processing},
  booktitle = {2013 14th International Workshop on Image Analysis for Multimedia Interactive Services (WIAMIS)},
  year      = {2013},
  pages     = {1--4},
  month     = jul,
  abstract  = {In the last decade there has been an explosion in the availability of digitized music material, which comprises data of various formats and modalities including textual, symbolic, acoustic and visual rep-resentations. For example, in the case of an opera there typically exist digitized versions of the libretto, different editions of the musical score, as well as a large number of performances given as audio and video recordings. In this paper, we give an overview of various informed approaches to music processing, where the availability of multiple sources of music-related information is used for supporting and improving the analysis of music data. Considering the scenario of the opera “Der Freischüitz” by Carl Maria von Webera work of central musical importance, where one can draw upon a rich body of sources-we highlight how the identification and creation of cross-modal relationships are a key issue in multimedia processing.},
  doi       = {10.1109/WIAMIS.2013.6616168},
  file      = {:Mueller2013.pdf:PDF},
  groups    = {Previous reviews on similar topics},
  issn      = {2158-5873},
  keywords  = {audio coding;electronic music;multimedia communication;music;source separation;multimodal scenario;informed music processing;digitized music material;music related information;music data;cross modal relationships;multimedia processing;Music;Synchronization;Source separation;Instruments;Context;Multiple signal classification;Score-informed processing;segmentation;source separation;audio editing;alignment;music synchronization, rank5},
}

@Article{Maestre2017,
  author   = {Maestre, Esteban and Papiotis, Panagiotis and Marchini, Marco and Llimona, Quim and Mayor, Oscar and Pérez, Alfonso and Wanderley, Marcelo M.},
  title    = {Enriched Multimodal Representations of Music Performances: Online Access and Visualization},
  journal  = {British Journal of Earth Sciences Research},
  year     = {2017},
  volume   = {24},
  number   = {1},
  pages    = {24--34},
  month    = jan,
  issn     = {1070-986X},
  abstract = {The authors provide a first-person outlook on the technical challenges involved in the recording, analysis, archiving, and cloud-based interchange of multimodal string quartet performance data as part of a collaborative research project on ensemble music making. To facilitate the sharing of their own collection of multimodal recordings and extracted descriptors and annotations, they developed a hosting platform through which multimodal data (audio, video, motion capture, and derived signals) can be stored, visualized, annotated, and selectively retrieved via a web interface and a dedicated API. This article offers a twofold contribution: the authors open their collection of enriched multimodal recordings, the Quartet dataset, to the community, and they introduce and enable access to their multimodal data exchange platform and web application, the Repovizz system. This article is part of a special issue on multimedia technologies for enriched music.},
  doi      = {10.1109/MMUL.2017.3},
  file     = {:Maestre2017.pdf:PDF},
  groups   = {Explorative studies, Resources},
  keywords = {cloud computing;data visualisation;information retrieval;Internet;multimedia computing;music;enriched multimodal music performance representations;online visualization;online access;cloud-based multimodal string quartet performance data interchange;ensemble music making;multimodal recordings;descriptor extraction;descriptor annotation;multimodal data annotation;multimodal data visualisation;multimodal data storage;Web interface;selective data retrieval;quartet dataset;Repovizz system;multimedia technologies;Instruments;Music;Data visualization;Multiple signal classification;Context awareness;Microphones;Media;Collaborative work;Recording;music performance;audio;video;motion capture;multimodal data;string quartet;bowed string;web access;remote visualization;data exchange;data analysis;multimedia;Internet/web technologies;visualization, rank3},
}

@InProceedings{Li2015,
  author    = {Li, Pei-Ching and Su, Li and Yang, Yi-Hsuan and Su, Alvin W. Y.},
  title     = {Analysis of Expressive Musical Terms in Violin Using Score-informed and Expression-based Audio Features},
  booktitle = {Proceedings of the 16th International Society for Music Information Retrieval Conference, {ISMIR} 2015, M{\'{a}}laga, Spain, October 26-30, 2015},
  year      = {2015},
  editor    = {Meinard M{\"{u}}ller and Frans Wiering},
  pages     = {809--815},
  bibsource = {dblp computer science bibliography, https://dblp.org},

  file      = {:Li2015.pdf:PDF},
  groups    = {Explorative studies},
  keywords  = {rank4},

}

@InProceedings{Sentuerk2013,
  author    = {Sentürk, Sertan and Gulati, Sankalp and Serra, Xavier},
  title     = {Score Informed Tonic Identification for Makam Music of Turkey},
  booktitle = {Proceedings of the 14th International Society for Music Information Retrieval Conference, {ISMIR} 2013, Curitiba, Brazil, November 4-8, 2013},
  year      = {2013},
  editor    = {Alceu de Souza Britto Jr. and Fabien Gouyon and Simon Dixon},
  pages     = {175--180},
  bibsource = {dblp computer science bibliography, https://dblp.org},

  file      = {:Sentuerk2013.pdf:PDF},
  groups    = {Explorative studies},
  keywords  = {rank4},

}

@Article{Kwon2017,
  author        = {Kwon, Taegyun and Jeong, Dasaem and Nam, Juhan},
  title         = {Audio-to-score Alignment of Piano Music Using Rnn-based Automatic Music Transcription},
  journal       = {Food Biology},
  year          = {2017},
  volume        = {abs/1711.04480},
  archiveprefix = {arXiv},
  bibsource     = {dblp computer science bibliography, https://dblp.org},

  eprint        = {1711.04480},
  file          = {:Kwon2017.pdf:PDF},
  groups        = {Audio-to-score alignment},
  keywords      = {rank4},

}

@Book{Bader2018,
  title     = {Springer Handbook of Systematic Musicology},
  publisher = {Springer Berlin Heidelberg},
  year      = {2018},
  editor    = {Rolf Bader},
  doi       = {10.1007/978-3-662-55004-5},
  file      = {:Bader2018.pdf:PDF},
  groups    = {PhD Milan},
}

@Article{Marenco2015,
  author    = {Marenco, Bernardo and Fuentes, Magdalena and Lanzaro, Florencia and Rocamora, Martín and Gómez, Alvaro},
  title     = {A Multimodal Approach for Percussion Music Transcription from Audio and Video},
  journal   = {Progress in Pattern Recognition, Image Analysis, Computer Vision, and Applications},
  year      = {2015},
  month     = jan,
  abstract  = {A multimodal approach for percussion music transcription from audio and video recordings is proposed in this work. It is part of an ongoing research effort for the development of tools for computer-aided analysis of Candombe drumming, a popular afro-rooted rhythm from Uruguay. Several signal processing techniques are applied to automatically extract meaningful information from each source. This involves detecting certain relevant objects in the scene from the video stream. The location of events is obtained from the audio signal and this information is used to drive the processing of both modalities. Then, the detected events are classified by combining the information from each source in a feature-level fusion scheme. The experiments conducted yield promising results that show the advantages of the proposed method.},
  date      = {2015-01-01},
  doi       = {10.1007/978-3-319-25751-8_12},
  file      = {:Marenco2015.pdf:PDF},
  groups    = {Explorative studies},
  keywords  = {rank3},
  publisher = {Springer},
}

@InProceedings{Ohkita2015,
  author    = {Ohkita, M. and Bando, Y. and Ikemiya, Y. and Itoyama, K. and Yoshii, K.},
  title     = {Audio-visual Beat Tracking Based on a State-space Model for a Music Robot Dancing with Humans},
  booktitle = {Proc. IEEE/RSJ Int. Conf. Intelligent Robots and Systems (IROS)},
  year      = {2015},
  pages     = {5555--5560},
  month     = sep,
  doi       = {10.1109/IROS.2015.7354164},
  file      = {:Ohkita2015.pdf:PDF},
  groups    = {Beat tracking},
  keywords  = {acoustic signal processing, audio signal processing, audio-visual systems, entertainment, feature extraction, humanoid robots, human-robot interaction, legged locomotion, microphones, music, particle filtering (numerical methods), probability, robot vision, state-space methods, F-measure value, Kinect depth sensor, multimodal method, particle filter, skeleton feature extraction, onset likelihoods, audio tempos, acoustic feature extraction, probabilistic method, state-space model, reverberant sounds, loud environmental noise, music audio signal recording, beat time detection, beat time prediction, human dancing movements, music audio signals, music-human dancer. synchronization, entertainment robot, audio-visual beat-tracking method, music robot, Feature extraction, Robots, Skeleton, Multiple signal classification, Visualization, Acoustics, State-space methods, rank4},
}

@InProceedings{Schindler2015,
  author    = {Schindler, Alexander and Rauber, Andreas},
  title     = {An Audio-visual Approach to Music Genre Classification through Affective Color Features},
  booktitle = {Advances in Information Retrieval},
  year      = {2015},
  editor    = {Hanbury, Allan and Kazai, Gabriella and Rauber, Andreas and Fuhr, Norbert},
  pages     = {61--67},
  address   = {Cham},
  publisher = {Springer International Publishing},
  abstract  = {This paper presents a study on classifying music by affective visual information extracted frommusic videos. The proposed audio-visual approach analyzes genre specific utilization of color. A comprehensive set of color specific image processing features used for affect and emotion recognition derived from psychological experiments or art-theory is evaluated in the visual and multi-modal domain against contemporary audio content descriptors. The evaluation of the presented color features is based on comparative classification experiments on the newly introduced `Music Video Dataset'. Results show that a combination of the modalities can improve non-timbral and rhythmic features but show insignificant effects on high performing audio features.},
  file      = {:Schindler2015.pdf:PDF},
  groups    = {Explorative studies},
  isbn      = {978-3-319-16354-3},
  keywords  = {rank3},
}

@Misc{Lim2011,
  author   = {Lim, Angelica and Nakamura, Keisuke and Nakadai, Kazuhiro and Ogata, Tetsuya and Okuno, Hiroshi G.},
  title    = {Audio-visual Musical Instrument Recognition},
  year     = {2011},
  abstract = {In 2008, a humanoid robot developed by Honda conducted a symphonic orchestra in front of a live audience. It per-fectly imitated the pre-recorded actions of the orchestra’s human conductor. Despite its realistic movements, it},
  file     = {:Lim2011.pdf:PDF},
  groups   = {Explorative studies},
  keywords = {rank3},
}

@InCollection{Fujihara2012,
  author    = {Fujihara, Hiromasa and Goto, Masataka},
  title     = {{Lyrics-to-Audio Alignment and its Application}},
  booktitle = {Multimodal Music Processing},
  publisher = {Schloss Dagstuhl--Leibniz-Zentrum fuer Informatik},
  year      = {2012},
  editor    = {Meinard M{\"u}ller and Masataka Goto and Markus Schedl},
  volume    = {3},
  series    = {Dagstuhl Follow-Ups},
  pages     = {23--36},
  address   = {Dagstuhl, Germany},
  isbn      = {978-3-939897-37-8},
  annote    = {Keywords: Lyrics, Alignment, Karaoke, Multifunctional music player, Lyrics-based music retrieval},
  doi       = {10.4230/DFU.Vol3.11041.23},
  file      = {:Fujihara2012.pdf:PDF},
  groups    = {Lyrics-to-audio},
  issn      = {1868-8977},

  urn       = {urn:nbn:de:0030-drops-34644},
}

@Article{Yang2012,
  author     = {Yang, Yi-Hsuan and Chen, Homer H.},
  title      = {Machine Recognition of Music Emotion: A Review},
  journal    = {Acm Trans. Intell. Syst. Technol.},
  year       = {2012},
  volume     = {3},
  number     = {3},
  pages      = {40:1--40:30},
  month      = may,
  issn       = {2157-6904},
  acmid      = {2168754},
  address    = {New York, NY, USA},
  articleno  = {40},
  doi        = {10.1145/2168752.2168754},
  file       = {:Yang2012.pdf:PDF},
  groups     = {Emotion/mood recognition},
  issue_date = {May 2012},
  keywords   = {Music emotion recognition},
  numpages   = {30},
  publisher  = {ACM},
}

@InProceedings{Mayer2011,
  author    = {Mayer, Rudolf and Rauber, Andreas},
  title     = {Musical Genre Classification by Ensembles of Audio and Lyrics Features},
  booktitle = {Proceedings of the 12th International Society for Music Information Retrieval Conference (ISMIR 2012)},
  year      = {2011},
  pages     = {675--680},
  publisher = {University of Miami},
  note      = {Vortrag: 12th International Society for Music Information Retrieval Conference (ISMIR 2012), Miami; 2011-10-24 -- 2011-10-28},
  abstract  = {Algorithms that can understand and interpret characteristics of music, and organise them for and recommend them to their users can be of great assistance in handling the ever growing size of both private and commercial collections.
Music is an inherently multi-modal type of data, and the lyrics associated with the music are as essential to the reception and the message of a song as is the audio. In this paper, we present advanced methods on how the lyrics domain of music can be combined with the acoustic domain. We evaluate our approach by means of a common task in music information retrieval, musical genre classification. Advancing over previous work that showed improvements with simple feature fusion, we apply the more sophisticated approach of result (or late) fusion. We achieve results superior to the best choice of a single algorithm on a single feature set.},
  file      = {:Mayer2011.pdf:PDF},
  groups    = {Genre recognition},
  isbn      = {978-0-615-54865-4},

}

@Article{Zhong2012,
  author  = {Zhong, Jiang and Cheng, Yifeng and Yang, Siyuan and Wen, Luosheng},
  title   = {Music Sentiment Classification Integrating Audio with Lyrics},
  journal = {Journal of Information \& Computational Science},
  year    = {2012},
  volume  = {9},
  number  = {1},
  pages   = {35--44},
  file    = {:Zhong2012.pdf:PDF},
  groups  = {Emotion/mood recognition},
}

@Article{Correya2018,
  author        = {Correya, Albin and Hennequin, Romain and Arcos, Mickaël},
  title         = {{Large-Scale Cover Song Detection in Digital Music Libraries Using Metadata, Lyrics and Audio Features}},
  journal       = {Arxiv E-prints},
  year          = {2018},
  month         = aug,
  adsnote       = {Provided by the SAO/NASA Astrophysics Data System},

  archiveprefix = {arXiv},
  eprint        = {1808.10351},
  file          = {:Correya2018.pdf:PDF},
  groups        = {Explorative studies},
  keywords      = {Computer Science - Information Retrieval, Computer Science - Multimedia, rank3},
  primaryclass  = {cs.IR},
}

@InCollection{Xue2015,
  author    = {Xue, Hao and Xue, Like and Su, Feng},
  title     = {Multimodal Music Mood Classification by Fusion of Audio and Lyrics},
  booktitle = {{MultiMedia} Modeling},
  publisher = {Springer International Publishing},
  year      = {2015},
  pages     = {26--37},
  doi       = {10.1007/978-3-319-14442-9_3},
  file      = {:Xue2015.pdf:PDF},
  groups    = {Emotion/mood recognition},
}

@InProceedings{Jeon2017,
  author    = {Jeon, Byungsoo and Kim, Chanju and Kim, Adrian and Kim, Dongwon and Park, Jangyeon and Ha, JungWoo},
  title     = {Music Emotion Recognition Via End-to-end Multimodal Neural Networks},
  booktitle = {Proceedings of the Poster Track of the 11th {ACM} Conference on Recommender Systems (RecSys 2017), Como, Italy, August 28, 2017.},
  year      = {2017},
  editor    = {Domonkos Tikk and Pearl Pu},
  volume    = {1905},
  series    = {{CEUR} Workshop Proceedings},
  publisher = {CEUR-WS.org},
  bibsource = {dblp computer science bibliography, https://dblp.org},

  file      = {:Jeon2017.pdf:PDF},
  groups    = {Emotion/mood recognition},
}

@InProceedings{Aryafar2014,
  author    = {Aryafar, Kamelia and Shokoufandeh, Ali},
  title     = {Multimodal Music and Lyrics Fusion Classifier for Artist Identification},
  booktitle = {2014 13th International Conference on Machine Learning and Applications},
  year      = {2014},
  month     = dec,
  publisher = {{IEEE}},
  doi       = {10.1109/icmla.2014.88},
  file      = {:Aryafar2014.pdf:PDF},
  groups    = {Explorative studies},
  keywords  = {rank5},
}

@Article{Schuller2010,
  author    = {Schuller, Björn and Hage, Clemens and Schuller, Dagmar and Rigoll, Gerhard},
  title     = {`mister D.j., Cheer Me Up!':musical and Textual Features for Automatic Mood Classification},
  journal   = {Journal of New Music Research},
  year      = {2010},
  volume    = {39},
  number    = {1},
  pages     = {13--34},
  month     = mar,
  doi       = {10.1080/09298210903430475},
  file      = {:Schuller2010.pdf:PDF},
  groups    = {Emotion/mood recognition},
  publisher = {Informa {UK} Limited},
}

@InProceedings{Kim2010,
  author    = {Kim, Youngmoo E. and Schmidt, Erik M. and Migneco, Raymond and Morton, Brandon G. and Richardson, Patrick and Scott, Jeffrey J. and Speck, Jacquelin A. and Turnbull, Douglas},
  title     = {State of the Art Report: Music Emotion Recognition: {A} State of the Art Review},
  booktitle = {Proceedings of the 11th International Society for Music Information Retrieval Conference, {ISMIR} 2010, Utrecht, Netherlands, August 9-13, 2010},
  year      = {2010},
  editor    = {J. Stephen Downie and Remco C. Veltkamp},
  pages     = {255--266},
  publisher = {International Society for Music Information Retrieval},
  bibsource = {dblp computer science bibliography, https://dblp.org},

  file      = {:Kim2010.pdf:PDF},
  groups    = {Emotion/mood recognition},

}

@InProceedings{Zhen2010,
  author    = {Zhen, Chao and Xu, Jieping},
  title     = {Multi-modal Music Genre Classification Approach},
  booktitle = {Proc. 3rd Int. Conf. Computer Science and Information Technology},
  year      = {2010},
  volume    = {8},
  pages     = {398--402},
  month     = jul,
  doi       = {10.1109/ICCSIT.2010.5564489},
  file      = {:Zhen2010.pdf:PDF},
  groups    = {Genre recognition},
  keywords  = {content-based retrieval, feature extraction, music, musical acoustics, pattern classification, probability, Web sites, multimodal music genre classification, music information retrieval system, acoustic feature, social tag, audio content based classification, IBFFS, interaction based forward feature selection, music tag, artist tag, Website, Latent Dirichlet allocation, generative probabilistic model, Computational modeling, Acoustics, Art, Atmospheric modeling, Analytical models, Neodymium, music genre classification, music tag, artist tag, IBFFS, LDA},
}

@InProceedings{Li2004,
  author    = {Li, Tao and Ogihara, Mitsunori},
  title     = {Music Artist Style Identification by Semi-supervised Learning from Both Lyrics and Content},
  booktitle = {Proceedings of the 12th Annual ACM International Conference on Multimedia},
  year      = {2004},
  series    = {MULTIMEDIA '04},
  pages     = {364--367},
  address   = {New York, NY, USA},
  publisher = {ACM},
  abstract  = {Efficient and intelligent music information retrieval is a very important topic of the 21st century. With the ultimate goal of building personal music information retrieval systems, this paper studies the problem of identifying "similar" artists using both lyrics and acoustic data. The approach for using a small set of labeled samples for the seed labeling to build classifiers that improve themselves using unlabeled data is presented. This approach is tested on a data set consisting of 43 artists and 56 albums using artist similarity provided by All Music Guide. Experimental results show that using such an approach the accuracy of artist similarity classifiers can be significantly improved and that artist similarity can be efficiently identified.},
  acmid     = {1027612},
  doi       = {10.1145/1027527.1027612},
  file      = {:Li2004.pdf:PDF},
  groups    = {Genre recognition},
  isbn      = {1-58113-893-8},
  keywords  = {artist style, lyrics, semi-supervised learning},
  location  = {New York, NY, USA},
  numpages  = {4},
}

@InProceedings{Mayer2008,
  author    = {Mayer, Rudolf and Neumayer, Robert and Rauber, Andreas},
  title     = {Combination of Audio and Lyrics Features for Genre Classification in Digital Audio Collections},
  booktitle = {Proceedings of the 16th ACM International Conference on Multimedia},
  year      = {2008},
  series    = {MM '08},
  pages     = {159--168},
  address   = {New York, NY, USA},
  publisher = {ACM},
  abstract  = {In many areas multimedia technology has made its way into mainstream. In the case of digital audio this is manifested in numerous online music stores having turned into profitable businesses. The widespread user adaption of digital audio both on home computers and mobile players show the size of this market. Thus, ways to automatically process and handle the growing size of private and commercial collections become increasingly important; along goes a need to make music interpretable by computers. The most obvious representation of audio files is their sound - there are, however, more ways of describing a song, for instance its lyrics, which describe songs in terms of content words. Lyrics of music may be orthogonal to its sound, and differ greatly from other texts regarding their (rhyme) structure. Consequently, the exploitation of these properties has potential for typical music information retrieval tasks such as musical genre classification; so far, there is a lack of means to efficiently combine these modalities. In this paper, we present findings from investigating advanced lyrics features such as the frequency of certain rhyme patterns, several parts-of-speech features, and statistic features such as words per minute (WPM). We further analyse in how far a combination of these features with existing acoustic feature sets can be exploited for genre classification and provide experiments on two test collections.},
  acmid     = {1459382},
  doi       = {10.1145/1459359.1459382},
  file      = {:Mayer2008.pdf:PDF},
  groups    = {Genre recognition},
  isbn      = {978-1-60558-303-7},
  keywords  = {audio features, feature fusion, feature selection, genre classification, lyrics processing, supervised learning},
  location  = {Vancouver, British Columbia, Canada},
  numpages  = {10},
}

@InProceedings{Hu2010,
  author    = {Hu, Xiao and Downie, J. Stephen},
  title     = {Improving Mood Classification in Music Digital Libraries by Combining Lyrics and Audio},
  booktitle = {Proceedings of the 10th Annual Joint Conference on Digital Libraries},
  year      = {2010},
  series    = {JCDL '10},
  pages     = {159--168},
  address   = {New York, NY, USA},
  publisher = {ACM},
  abstract  = {Mood is an emerging metadata type and access point in music digital libraries (MDL) and online music repositories. In this study, we present a comprehensive investigation of the usefulness of lyrics in music mood classification by evaluating and comparing a wide range of lyric text features including linguistic and text stylistic features. We then combine the best lyric features with features extracted from music audio using two fusion methods. The results show that combining lyrics and audio significantly outperformed systems using audio-only features. In addition, the examination of learning curves shows that the hybrid lyric + audio system needed fewer training samples to achieve the same or better classification accuracies than systems using lyrics or audio singularly. These experiments were conducted on a unique large-scale dataset of 5,296 songs (with both audio and lyrics for each) representing 18 mood categories derived from social tags. The findings push forward the state-of-the-art on lyric sentiment analysis and automatic music mood classification and will help make mood a practical access point in music digital libraries.},
  acmid     = {1816146},
  doi       = {10.1145/1816123.1816146},
  groups    = {Emotion/mood recognition},
  isbn      = {978-1-4503-0085-8},
  keywords  = {audio features, feature fusion, lyric sentiment analysis, music digital libraries, music mood classification, supervised learning},
  location  = {Gold Coast, Queensland, Australia},
  numpages  = {10},
}

@Article{Atrey2010,
  author    = {Atrey, Pradeep K. and Hossain, M. Anwar and Saddik, Abdulmotaleb El and Kankanhalli, Mohan S.},
  title     = {Multimodal Fusion for Multimedia Analysis: A Survey},
  journal   = {Multimedia Systems},
  year      = {2010},
  volume    = {16},
  number    = {6},
  pages     = {345--379},
  month     = apr,
  doi       = {10.1007/s00530-010-0182-0},
  file      = {:Atrey2010.pdf:PDF},
  groups    = {Previous reviews on similar topics},
  keywords  = {rank5},
  publisher = {Springer Nature},
}

@InProceedings{Zhu2013,
  author    = {Zhu, Q. and Li, Z. and Wang, H. and Yang, Y. and Shyu, M.},
  title     = {Multimodal Sparse Linear Integration for Content-based Item Recommendation},
  journal = {AARF},
  year      = {2013},
  pages     = {187--194},
  month     = dec,
  abstract  = {Most content-based recommender systems focus on analyzing the textual information of items. For items with images, the images can be treated as another information modality. In this paper, an effective method called MSLIM is proposed to integrate multimodal information for content-based item recommendation. It formalizes the probelm into a regularized optimization problem in the least-squares sense and the coordinate gradient descent is applied to solve the problem. The aggregation coefficients of the items are learned in an unsupervised manner during this process, based on which the k-nearest neighbor (k-NN) algorithm is used to generate the top-N recommendations of each item by finding its k nearest neighbors. A framework of using MSLIM for item recommendation is proposed accordingly. The experimental results on a self-collected handbag dataset show that MSLIM outperforms the selected comparison methods and show how the model parameters affect the final recommendation results.},
  doi       = {10.1109/ISM.2013.37},
  file      = {:Zhu2013.pdf:PDF},
  groups    = {SLIM},
  keywords  = {content-based retrieval, learning (artificial intelligence), least squares approximations, pattern classification, recommender systems, multimodal sparse linear integration, content-based item recommendation, content-based recommender systems, textual information, information modality, MSLIM, multimodal information, regularized optimization problem, least-squares sense, coordinate gradient descent, aggregation coefficients, unsupervised manner, k-nearest neighbor algorithm, k-NN algorithm, top-N recommendation, k nearest neighbors, self-collected handbag dataset, model parameters, Feature extraction, Equations, Visualization, Mathematical model, Image color analysis, Sparse matrices, Multimedia communication, Recommendation, sparse linear, multimodal integration},
}

@Article{Zhu2015,
  author    = {Zhu, Qiusha and Shyu, Mei-Ling},
  title     = {Sparse Linear Integration of Content and Context Modalities for Semantic Concept Retrieval},
  journal   = {{IEEE} Transactions on Emerging Topics in Computing},
  year      = {2015},
  volume    = {3},
  number    = {2},
  pages     = {152--160},
  month     = jun,
  doi       = {10.1109/tetc.2014.2384992},
  file      = {:Zhu2015.pdf:PDF},
  groups    = {SLIM},
  publisher = {Institute of Electrical and Electronics Engineers ({IEEE})},
}

@InProceedings{Ning2011,
  author    = {Ning, Xia and Karypis, George},
  title     = {{SLIM}: Sparse Linear Methods for Top-n Recommender Systems},
  booktitle = {2011 {IEEE} 11th International Conference on Data Mining},
  year      = {2011},
  month     = dec,
  publisher = {{IEEE}},
  doi       = {10.1109/icdm.2011.134},
  file      = {:Ning2011.pdf:PDF},
  groups    = {SLIM},
}

@InProceedings{Ning2012,
  author    = {Ning, Xia and Karypis, George},
  title     = {Sparse Linear Methods with Side Information for Top-n Recommendations},
  booktitle = {Proceedings of the sixth {ACM} conference on Recommender systems - {RecSys} {\textquotesingle}12},
  year      = {2012},
  publisher = {{ACM} Press},
  doi       = {10.1145/2365952.2365983},
  file      = {:Ning2012.pdf:PDF},
  groups    = {SLIM},
}

@InProceedings{Smith2017,
  author    = {Smith, Jordan B. L. and Hamasaki, M. and Goto, M.},
  title     = {Classifying Derivative Works with Search, Text, Audio and Video Features},
  booktitle = {Proc. IEEE Int. Conf. Multimedia and Expo (ICME)},
  year      = {2017},
  pages     = {1422--1427},
  month     = jul,
  abstract  = {Users of video-sharing sites often search for derivative works of music, such as live versions, covers, and remixes. Audio and video content are both important for retrieval: “karaoke” specifies audio content (instrumental version) and video content (animated lyrics). Although YouTube's text search is fairly reliable, many search results do not match the exact query. We introduce an algorithm to classify YouTube videos by category of derivative work. Based on a standard pipeline for video-based genre classification, it combines search, text, and video features with a novel set of audio features derived from audio fingerprints. A baseline approach is outperformed by the search and text features alone, and combining these with video and audio features performs best of all, reducing the audio content error rate from 25\% to 15\%.},
  doi       = {10.1109/ICME.2017.8019444},
  file      = {:Smith2017.pdf:PDF},
  groups    = {Explorative studies},
  issn      = {1945-788X},
  keywords  = {feature extraction, image classification, social networking (online), video signal processing, derivative works classification, search feature, text feature, audio feature, video feature, video-sharing sites, instrumental version, animated lyrics, YouTube, Music, Instruments, Image color analysis, Feature extraction, Multimedia communication, Streaming media, content classification, derivative works, fingerprinting, multimedia retrieval, rank3},
}

@Article{Alias2016,
  author    = {Alías, Francesc and Socoró, Joan and Sevillano, Xavier},
  title     = {A Review of Physical and Perceptual Feature Extraction Techniques for Speech, Music and Environmental Sounds},
  journal   = {Applied Sciences},
  year      = {2016},
  volume    = {6},
  number    = {5},
  pages     = {143},
  month     = may,
  doi       = {10.3390/app6050143},
  file      = {:Alias2016.pdf:PDF},
  groups    = {PhD Milan},
  publisher = {{MDPI} {AG}},
}

@Article{Hu2016,
  author    = {Hu, Xiao and Choi, Kahyun and Downie, J. Stephen},
  title     = {A Framework for Evaluating Multimodal Music Mood Classification},
  journal   = {Journal of the Association for Information Science and Technology},
  year      = {2016},
  volume    = {68},
  number    = {2},
  pages     = {273--285},
  month     = jan,
  abstract  = {This research proposes a framework for music mood classification that uses multiple and complementary information sources, namely, music audio, lyric text, and social tags associated with music pieces. This article presents the framework and a thorough evaluation of each of its components. Experimental results on a large data set of 18 mood categories show that combining lyrics and audio significantly outperformed systems using audio-only features. Automatic feature selection techniques were further proved to have reduced feature space. In addition, the examination of learning curves shows that the hybrid systems using lyrics and audio needed fewer training samples and shorter audio clips to achieve the same or better classification accuracies than systems using lyrics or audio singularly. Last but not least, performance comparisons reveal the relative importance of audio and lyric features across mood categories.},
  doi       = {10.1002/asi.23649},
  file      = {:Hu2016.pdf:PDF},
  groups    = {Emotion/mood recognition},
  publisher = {Wiley},
}

@Article{Schindler2016,
  author    = {Schindler, Alexander and Rauber, Andreas},
  title     = {Harnessing Music-related Visual Stereotypes for Music Information Retrieval},
  journal   = {{ACM} Transactions on Intelligent Systems and Technology},
  year      = {2016},
  volume    = {8},
  number    = {2},
  pages     = {1--21},
  month     = oct,
  doi       = {10.1145/2926719},
  file      = {:Schindler2016.pdf:PDF},
  groups    = {Genre recognition},
  publisher = {Association for Computing Machinery ({ACM})},
}

@InProceedings{Sra2017,
  author    = {Sra, Misha and Maes, Pattie and Vijayaraghavan, Prashanth and Roy, Deb},
  title     = {Auris: Creating Affective Virtual Spaces from Music},
  booktitle = {Proceedings of the 23rd ACM Symposium on Virtual Reality Software and Technology},
  year      = {2017},
  series    = {VRST '17},
  pages     = {26:1--26:11},
  address   = {New York, NY, USA},
  publisher = {ACM},
  abstract  = {Affective virtual spaces are of interest in many virtual reality applications such as education, wellbeing, rehabilitation, and entertainment. In this paper we present Auris, a system that attempts to generate affective virtual environments from music. We use music as input because it inherently encodes emotions that listeners readily recognize and respond to. Creating virtual environments is a time consuming and labor-intensive task involving various skills like design, 3D modeling, texturing, animation, and coding. Auris helps make this easier by automating the virtual world generation task using mood and content extracted from song audio and lyrics data respectively. Our user study results indicate virtual spaces created by Auris successfully convey the mood of the songs used to create them and achieve high presence scores with the potential to provide novel experiences of listening to music.},
  acmid     = {3139139},
  articleno = {26},
  doi       = {10.1145/3139131.3139139},
  file      = {\:Sra2017.pdf\:PDF:\:Sra2017.pdf\:PDF:PDF;:Sra2017.pdf:PDF},
  groups    = {Resources},
  isbn      = {978-1-4503-5548-3},
  keywords  = {deep neural networks, generative models, music, virtual reality},
  location  = {Gothenburg, Sweden},
  numpages  = {11},
}

@InProceedings{Sprechmann2012,
  author    = {Sprechmann, P. and Cancela, P. and Sapiro, G.},
  title     = {Gaussian Mixture Models for Score-informed Instrument Separation},
  booktitle = {Proc. Speech and Signal Processing (ICASSP) 2012 IEEE Int. Conf. Acoustics},
  year      = {2012},
  pages     = {49--52},
  month     = mar,
  abstract  = {A new framework for representing quasi-harmonic signals, and its application to score-informed single channel musical instruments separation, is introduced in this paper. In the proposed approach, the signal's pitch and spectral envelope are modeled separately. The model combines parametric filters enforcing an harmonic structure in the representation, with Gaussian modeling for representing the spectral envelope. The estimation of the signal's model is cast as an inverse problem efficiently solved via a maximum a posteriori expectation-maximization algorithm. The relation of the proposed framework with common non-negative factorization methods is also discussed. The algorithm is evaluated with both real and synthetic instruments mixtures, and comparisons with recently proposed techniques are presented.},
  doi       = {10.1109/ICASSP.2012.6287814},
  file      = {\:Sprechmann2012.pdf\:PDF:\:Sprechmann2012.pdf\:PDF:PDF;:Sprechmann2012.pdf:PDF},
  groups    = {Score-informed source separation},
  issn      = {2379-190X},
  keywords  = {Gaussian processes, optimisation, signal representation, source separation, Gaussian mixture models, score-informed instrument separation, representing quasi-harmonic signals, score-informed single channel musical instruments separation, parametric filters, spectral envelope, inverse problem, maximum a posteriori expectation-maximization algorithm, nonnegative factorization methods, Instruments, Harmonic analysis, Source separation, Spectrogram, Estimation, Principal component analysis, Time frequency analysis, Score-informed source separation, single channel source separation, audio modeling},
}

@Article{Rodriguez-Serrano2015,
  author    = {Rodriguez-Serrano, Francisco J. and Duan, Zhiyao and Vera-Candeas, Pedro and Pardo, Bryan and Carabias-Orti, Julio J.},
  title     = {Online Score-informed Source Separation with Adaptive Instrument Models},
  journal   = {Journal of New Music Research},
  year      = {2015},
  volume    = {44},
  number    = {2},
  pages     = {83--96},
  month     = jan,
  doi       = {10.1080/09298215.2014.989174},
  file      = {\:Rodriguez-Serrano2015.pdf\:PDF:\:Rodriguez-Serrano2015.pdf\:PDF:PDF;:Rodriguez-Serrano2015.pdf:PDF},
  groups    = {Score-informed source separation},
  publisher = {Informa {UK} Limited},
}

@Article{Dittmar2012,
  author    = {Dittmar, Christian and Cano, Estefanía and Abeßer, Jakob and Grollmisch, Sascha},
  title     = {Music Information Retrieval Meets Music Education},
  year      = {2012},
  doi       = {10.4230/dfu.vol3.11041.95},
  file      = {\:Dittmar2012.pdf\:PDF:\:Dittmar2012.pdf\:PDF:PDF;:Dittmar2012.pdf:PDF},
  groups    = {Spatial transcription},
  keywords  = {Computer Science, 000 Computer science, knowledge, general works},
  language  = {eng},
  publisher = {Schloss Dagstuhl - Leibniz-Zentrum fuer Informatik GmbH, Wadern/Saarbruecken, Germany},
}

@Article{Li2017,
  author   = {Li, Bochen and Xu, Chenliang and Duan, Zhiyao},
  title    = {Audiovisual Source Association for String Ensembles through Multi-modal Vibrato Analysis},
  journal  = {Proc. Sound and Music Computing (smc)},
  year     = {2017},
  file     = {\:Li2017.pdf\:PDF:\:Li2017.pdf\:PDF:PDF;:Li2017.pdf:PDF},
  groups   = {Explorative studies},
  keywords = {rank3},
}

@InProceedings{Li2017a,
  author    = {Li, Bochen and Dinesh, Karthik and Duan, Zhiyao and Sharma, Gaurav},
  title     = {See and Listen: Score-informed Association of Sound Tracks to Players in Chamber Music Performance Videos},
  booktitle = {Proc. Speech and Signal Processing (ICASSP) 2017 IEEE Int. Conf. Acoustics},
  year      = {2017},
  pages     = {2906--2910},
  month     = mar,
  abstract  = {Both audio and visual aspects of a musical performance, especially their association, are important for expressing players' ideas and for engaging the audience. In this paper, we present a framework for combining audio and video analyses of multi-instrument chamber music performances to associate players in the video to the individual separated instrument sources from the audio, in a score-informed fashion. The instrument sources are first separated using a score-informed source separation techniques. The individual sources are then associated with different players in the video by correlating the onset instants of their aligned score tracks with the players' motion detected using optical flow. Experiments on 19 musical pieces with varying polyphony show that the proposed method obtains the correct association for 17 pieces, and an accuracy of 89.2\% of the association of all individual tracks. The approach enables novel music enjoyment experiences by allowing users to target an audio source by clicking on the player in the video to separate/enhance it.},
  doi       = {10.1109/ICASSP.2017.7952688},
  file      = {\:Li2017a.pdf\:PDF:\:Li2017a.pdf\:PDF:PDF;:Li2017a.pdf:PDF},
  groups    = {Explorative studies},
  issn      = {2379-190X},
  keywords  = {image sequences, music, source separation, video signal processing, score-informed association, sound tracks, chamber music performance videos, musical performance, audio analysis, video analysis, multi-instrument chamber music performance, Videos, Instruments, Music, Tracking, Source separation, Image motion analysis, Computer vision, Multi-modal music analysis, motion analysis, source association, audio-score alignment, rank4},
}

@InProceedings{Dinesh2017,
  author    = {Dinesh, K. and Li, B. and Liu, X. and Duan, Z. and Sharma, G.},
  title     = {Visually Informed Multi-pitch Analysis of String Ensembles},
  booktitle = {Proc. Speech and Signal Processing (ICASSP) 2017 IEEE Int. Conf. Acoustics},
  year      = {2017},
  pages     = {3021--3025},
  month     = mar,
  abstract  = {Multi-pitch analysis of polyphonic music requires estimating concurrent pitches (estimation) and organizing them into temporal streams according to their sound sources (streaming). This is challenging for approaches based on audio alone due to the polyphonic nature of the audio signals. Video of the performance, when available, can be useful to alleviate some of the difficulties. In this paper, we propose to detect the play/non-play (P/NP) activities from musical performance videos using optical flow analysis to help with audio-based multi-pitch analysis. Specifically, the detected P/NP activity provides a more accurate estimate of the instantaneous polyphony (i.e., the number of pitches at a time instant), and also helps with assigning pitch estimates to only active sound sources. As the first attempt towards audio-visual multi-pitch analysis of multi-instrument musical performances, we demonstrate the concept on 11 string ensembles. Experiments show a high overall P/NP detection accuracy of 85.3\%, and a statistically significant improvement on both the multi-pitch estimation and streaming accuracy, under paired t-tests at a significance level of 0:01 in most cases.},
  doi       = {10.1109/ICASSP.2017.7952711},
  file      = {\:Dinesh2017.pdf\:PDF:\:Dinesh2017.pdf\:PDF:PDF;:Dinesh2017.pdf:PDF},
  groups    = {Explorative studies},
  issn      = {2379-190X},
  keywords  = {image sequences, music, statistical testing, video signal processing, visually informed multipitch analysis, string ensembles, polyphonic music, temporal streams, sound sources, musical performance videos, optical flow analysis, P/NP activity detection, instantaneous polyphony estimation, multi-instrument musical performances, paired t-tests, Estimation, Feature extraction, Streaming media, Timbre, Integrated optics, Optical imaging, Support vector machines, Multi-pitch estimation, streaming, audio-visual analysis, source separation, constrained clustering, SVM classifier, rank4},
}

@Article{Izadinia2013,
  author   = {Izadinia, H. and Saleemi, I. and Shah, M.},
  title    = {Multimodal Analysis for Identification and Segmentation of Moving-sounding Objects},
  journal  = {Ieee Transactions on Multimedia},
  year     = {2013},
  volume   = {15},
  number   = {2},
  pages    = {378--390},
  month    = feb,
  issn     = {1520-9210},
  abstract = {In this paper, we propose a novel method that exploits correlation between audio-visual dynamics of a video to segment and localize objects that are the dominant source of audio. Our approach consists of a two-step spatiotemporal segmentation mechanism that relies on velocity and acceleration of moving objects as visual features. Each frame of the video is segmented into regions based on motion and appearance cues using the QuickShift algorithm, which are then clustered over time using K-means, so as to obtain a spatiotemporal video segmentation. The video is represented by motion features computed over individual segments. The Mel-Frequency Cepstral Coefficients (MFCC) of the audio signal, and their first order derivatives are exploited to represent audio. The proposed framework assumes there is a non-trivial correlation between these audio features and the velocity and acceleration of the moving and sounding objects. The canonical correlation analysis (CCA) is utilized to identify the moving objects which are most correlated to the audio signal. In addition to moving-sounding object identification, the same framework is also exploited to solve the problem of audio-video synchronization, and is used to aid interactive segmentation. We evaluate the performance of our proposed method on challenging videos. Our experiments demonstrate significant increase in performance over the state-of-the-art both qualitatively and quantitatively, and validate the feasibility and superiority of our approach.},
  doi      = {10.1109/TMM.2012.2228476},
  groups   = {Segmentation},
  keywords = {audio signal processing, image motion analysis, image representation, image segmentation, learning (artificial intelligence), object detection, pattern clustering, statistical analysis, synchronisation, video signal processing, multimodal analysis, moving-sounding object, object identification, object segmentation, audio-visual dynamics, video dynamics, audio dominant source, two-step spatiotemporal segmentation mechanism, moving object velocity, moving object acceleration, motion cue, appearance cue, QuickShift algorithm, K-means clustering, motion feature representation, Mel-frequency cepstral coefficients, canonical correlation analysis, audio signal correlation, audio-video synchronization, Visualization, Correlation, Motion segmentation, Mel frequency cepstral coefficient, Image segmentation, Feature extraction, Acceleration, Audio-visual analysis, audio-visual synchronization, canonical correlation analysis, video segmentation},
}

@InProceedings{Gregorio2016,
  author    = {Gregorio, Jeff and Kim, Youngmoo},
  title     = {Phrase-level Audio Segmentation of Jazz Improvisations Informed by Symbolic Data.},
  booktitle = {Proceedings of the 17th International Society for Music Information Retrieval Conference, {ISMIR} 2016, New York City, United States},
  year      = {2016},
  pages     = {482--487},
  file      = {\:Gregorio2016.pdf\:PDF:\:Gregorio2016.pdf\:PDF:PDF;:Gregorio2016.pdf:PDF},
  groups    = {Segmentation},
  keywords  = {rank4},
}

@InProceedings{Ewert2017,
  author    = {Ewert, Sebastian and Sandler, Mark B.},
  title     = {An Augmented Lagrangian Method for Piano Transcription Using Equal Loudness Thresholding and Lstm-based Decoding},
  booktitle = {International Conference on Business Management \& Information Systems},
  year      = {2017},
  month     = oct,
  publisher = {{IEEE}},
  doi       = {10.1109/waspaa.2017.8170012},
  file      = {\:Ewert2017.pdf\:PDF:\:Ewert2017.pdf\:PDF:PDF;:Ewert2017.pdf:PDF},
  groups    = {Resources},
}

@InProceedings{Zhang2009,
  author    = {Zhang, Bingjun and Shen, Jialie and Xiang, Qiaoliang and Wang, Ye},
  title     = {Compositemap: A Novel Framework for Music Similarity Measure},
  booktitle = {Proceedings of the 32Nd International ACM SIGIR Conference on Research and Development in Information Retrieval},
  year      = {2009},
  series    = {SIGIR '09},
  pages     = {403--410},
  address   = {New York, NY, USA},
  publisher = {ACM},
  abstract  = {With the continuing advances in data storage and communication technology, there has been an explosive growth of music information from different application domains. As an effective technique for organizing, browsing, and searching large data collections, music information retrieval is attracting more and more attention. How to measure and model the similarity between different music items is one of the most fundamental yet challenging research problems. In this paper, we introduce a novel framework based on a multimodal and adaptive similarity measure for various applications. Distinguished from previous approaches, our system can effectively combine music properties from different aspects into a compact signature via supervised learning. In addition, an incremental Locality Sensitive Hashing algorithm has been developed to support efficient retrieval processes with different kinds of queries. Experimental results based on two large music collections reveal various advantages of the proposed framework including effectiveness, efficiency, adaptiveness, and scalability.},
  acmid     = {1572011},
  doi       = {10.1145/1571941.1572011},
  file      = {\:Zhang2009.pdf\:PDF:\:Zhang2009.pdf\:PDF:PDF;:Zhang2009.pdf:PDF},
  groups    = {Multimodal queries},
  isbn      = {978-1-60558-483-6},
  keywords  = {browsing, music, personalization, recommendation, search, similarity measure},
  location  = {Boston, MA, USA},
  numpages  = {8},
}

@Article{Levy2009,
  author   = {Levy, M. and Sandler, M.},
  title    = {Music Information Retrieval Using Social Tags and Audio},
  journal  = {Ieee Transactions on Multimedia},
  year     = {2009},
  volume   = {11},
  number   = {3},
  pages    = {383--395},
  month    = apr,
  issn     = {1520-9210},
  abstract = {In this paper we describe a novel approach to applying text-based information retrieval techniques to music collections. We represent tracks with a joint vocabulary consisting of both conventional words, drawn from social tags, and audio $<$i$>$muswords$<$/i$>$, representing characteristics of automatically-identified regions of interest within the signal. We build vector space and latent aspect models indexing words and muswords for a collection of tracks, and show experimentally that retrieval with these models is extremely well-behaved. We find in particular that retrieval performance remains good for tracks by artists unseen by our models in training, and even if tags for their tracks are extremely sparse.},
  doi      = {10.1109/TMM.2009.2012913},
  file     = {\:Levy2009.pdf\:PDF:\:Levy2009.pdf\:PDF:PDF;:Levy2009.pdf:PDF},
  groups   = {Multimodal queries},
  keywords = {audio signal processing, indexing, information retrieval, music, text analysis, text-based music information retrieval, social tag, vocabulary, vector space, latent aspect model, audio indexing, Music information retrieval, Vocabulary, Indexing, Collaborative work, Filtering, Navigation, Fingerprint recognition, Audio recording, Mood, Recommender systems, Audio, information retrieval, music, social tags},
}

@PhdThesis{Zhonghua2013,
  author = {Zhonghua, Li},
  title  = {Multimodal Music Information Retrieval: From Content Analysis to Multimodal Fusion},
  year   = {2013},
  file   = {:Zhonghua2013.pdf:PDF},
  groups = {Multimodal queries},
}

@InProceedings{Schedl2013a,
  author    = {Schedl, Markus and Orio, Nicola and Liem, Cynthia C. S. and Peeters, Geoffroy},
  title     = {A Professionally Annotated and Enriched Multimodal Data Set on Popular Music},
  booktitle = {Proceedings of the 4th ACM Multimedia Systems Conference},
  year      = {2013},
  series    = {MMSys '13},
  pages     = {78--83},
  address   = {New York, NY, USA},
  publisher = {ACM},
  abstract  = {This paper presents the MusiClef data set, a multimodal data set of professionally annotated music. It includes editorial metadata about songs, albums, and artists, as well as MusicBrainz identifiers to facilitate linking to other data sets. In addition, several state-of-the-art audio features are provided. Different sets of annotations and music context data -- collaboratively generated user tags, web pages about artists and albums, and the annotation labels provided by music experts -- are included too. Versions of this data set were used in the MusiClef evaluation campaigns in 2011 and 2012 for auto-tagging tasks. We report on the motivation for the data set, on its composition, on related sets, and on the evaluation campaigns in which versions of the set were already used. These campaigns likewise represent one use case, i.e. music auto-tagging, of the data set. The complete data set is publicly available for download at http://www.cp.jku.at/musiclef.},
  acmid     = {2483985},
  doi       = {10.1145/2483977.2483985},
  file      = {\:Schedl2013a.pdf\:PDF:\:Schedl2013a.pdf\:PDF:PDF;:Schedl2013a.pdf:PDF},
  groups    = {Resources},
  isbn      = {978-1-4503-1894-5},
  keywords  = {MusiClef, multimodal music data sets},
  location  = {Oslo, Norway},
  numpages  = {6},
}

@InProceedings{Bell2007,
  author    = {Bell, Bo and Kleban, Jim and Overholt, Dan and Putnam, Lance and Thompson, John and Kuchera-Morin, JoAnn},
  title     = {The Multimodal Music Stand},
  booktitle = {Proceedings of the 7th International Conference on New Interfaces for Musical Expression},
  year      = {2007},
  series    = {NIME '07},
  pages     = {62--65},
  address   = {New York, NY, USA},
  publisher = {ACM},
  abstract  = {We present the Multimodal Music Stand (MMMS) for the untethered sensing of performance gestures and the interactive control of music. Using e-field sensing, audio analysis, and computer vision, the MMMS captures a performer's continuous expressive gestures and robustly identifies discrete cues in a musical performance. Continuous and discrete gestures are sent to an interactive music system featuring custom designed software that performs real-time spectral transformation of audio.},
  acmid     = {1279750},
  doi       = {10.1145/1279740.1279750},
  file      = {\:Bell2007.pdf\:PDF:\:Bell2007.pdf\:PDF:PDF;:Bell2007.pdf:PDF},
  groups    = {Resources},
  keywords  = {computer vision, e-field sensing, interactivity, multimodal, untethered control},
  location  = {New York, New York},
  numpages  = {4},
}

@InProceedings{Schuller2004,
  author    = {Schuller, B. and Rigoll, G. and Lang, M.},
  title     = {Multimodal Music Retrieval for Large Databases},
  booktitle = {Proc. IEEE Int. Conf. Multimedia and Expo (ICME) (IEEE Cat. No.04TH8763)},
  year      = {2004},
  volume    = {2},
  pages     = {755--758 Vol.2},
  month     = jun,
  abstract  = {We present a novel multi-modal access to large MP3 music databases. Retrieval can be fulfilled either in a content-based manner or by keywords. As input modalities, speech by natural language utterances or singing, and manual interaction by handwriting, typing or hardkeys are used. In order to achieve especially robust retrieval results and automatically suggest music to the user, contextual knowledge of the time, date, season, user emotion, and listening habits is integrated in the retrieval process. The system communicates with the user by speech or visual reactions. The concepts shown are especially designed for home and mobile access on tablet-PCs, PDAs, and similar PC solutions, The paper discusses the concept and a working prototype called Shangrila. An evaluation by a user study leads to an impression of the capabilities of the suggested approach to multimodal music retrieval.},
  doi       = {10.1109/ICME.2004.1394310},
  file      = {\:Schuller2004.pdf\:PDF:\:Schuller2004.pdf\:PDF:PDF;:Schuller2004.pdf:PDF},
  groups    = {Multimodal queries},
  keywords  = {music, audio databases, content-based retrieval, natural language interfaces, graphical user interfaces, multimodal music retrieval, large music databases, MP3 music databases, content-based retrieval, keywords, natural language utterances, singing, handwriting, typing, hardkeys, contextual knowledge, user emotion, listening habits, tablet-PC, PDA, GUI, graphical user interface, Music information retrieval, Speech, Digital audio players, Audio databases, Content based retrieval, Natural languages, Robustness, Context, Personal digital assistants, Prototypes},
}

@InProceedings{Cheng_2009,
  author    = {Cheng, Heng-Tze and Yang, Yi-Hsuan and Lin, Yu-Ching and Chen, Homer H.},
  title     = {Multimodal Structure Segmentation and Analysis of Music Using Audio and Textual Information},
  booktitle = {2009 {IEEE} International Symposium on Circuits and Systems},
  year      = {2009},
  month     = may,
  publisher = {{IEEE}},
  doi       = {10.1109/iscas.2009.5118096},
  groups    = {Segmentation},
}

@InProceedings{Zhu2005,
  author    = {Zhu, Yongwei and Chen, Kai and Sun, Qibin},
  title     = {Multimodal Content-based Structure Analysis of Karaoke Music},
  booktitle = {Proceedings of the 13th Annual ACM International Conference on Multimedia},
  year      = {2005},
  series    = {MULTIMEDIA '05},
  pages     = {638--647},
  address   = {New York, NY, USA},
  publisher = {ACM},
  abstract  = {This paper presents a novel approach for content-based analysis of karaoke music, which utilizes multimodal contents including synchronized lyrics text from the video channel and original singing audio as well as accompaniment audio in the two audio channels. We proposed a novel video text extraction technique to accurately segment the bitmaps of lyrics text from the video frames and track the time of its color changes that are synchronized to the music. A technique that characterizes the original singing voice by analyzing the volume balance between the two audio channels is also proposed. A novel music structure analysis method using lyrics text and audio content is then proposed to precisely identify the verses and choruses of a song, and segment the lyrics into singing phrases. Experimental results based on 20 karaoke music titles of difference languages have shown that our proposed video text extraction technique can detect and segment the lyrics texts with accuracy higher than 90%, and the proposed multimodal approach for music structure analysis method has better performance than the previous methods that are based only on audio content analysis.},
  acmid     = {1101293},
  doi       = {10.1145/1101149.1101293},
  groups    = {Segmentation},
  isbn      = {1-59593-044-2},
  keywords  = {karaoke, multimodality, music information retrieval, music structure analysis, video text detection},
  location  = {Hilton, Singapore},
  numpages  = {10},
}

@InCollection{Camurri_2004,
  author    = {Camurri, Antonio and Mazzarino, Barbara and Ricchetti, Matteo and Timmers, Renee and Volpe, Gualtiero},
  title     = {Multimodal Analysis of Expressive Gesture in Music and Dance Performances},
  booktitle = {Gesture-Based Communication in Human-Computer Interaction},
  publisher = {Springer Berlin Heidelberg},
  year      = {2004},
  pages     = {20--39},
  doi       = {10.1007/978-3-540-24598-8_3},
  file      = {\:Camurri_2004.pdf\:PDF:\:Camurri_2004.pdf\:PDF:PDF;:Camurri_2004.pdf:PDF},
  groups    = {Segmentation},
}

@InCollection{Camurri2011,
  author    = {Camurri, Antonio and Volpe, Gualtiero},
  title     = {Multimodal Analysis of Expressive Gesture in Music Performance},
  booktitle = {Springer Tracts in Advanced Robotics},
  publisher = {Springer Berlin Heidelberg},
  year      = {2011},
  pages     = {47--66},
  doi       = {10.1007/978-3-642-22291-7_4},
  file      = {\:Camurri2011.pdf\:PDF:\:Camurri2011.pdf\:PDF:PDF;:Camurri2011.pdf:PDF},
  groups    = {Resources},
}

@InProceedings{Slizovskaia2017,
  author    = {Slizovskaia, Olga and Gómez, Emilia and Haro, Gloria},
  title     = {Musical Instrument Recognition in User-generated Videos Using a Multimodal Convolutional Neural Network Architecture},
  booktitle = {Proceedings of the 2017 {ACM} on International Conference on Multimedia Retrieval - {ICMR} {\textquotesingle}17},
  year      = {2017},
  publisher = {{ACM} Press},
  doi       = {10.1145/3078971.3079002},
  file      = {\:Slizovskaia2017.pdf\:PDF:\:Slizovskaia2017.pdf\:PDF:PDF;:Slizovskaia2017.pdf:PDF},
  groups    = {Explorative studies},
  keywords  = {rank4},
}

@Article{Suyoto2008,
  author    = {Suyoto, Iman S. H. and Uitdenbogerd, Alexandra L. and Scholer, Falk},
  title     = {Searching Musical Audio Using Symbolic Queries},
  journal   = {{IEEE} Transactions on Audio, Speech, and Language Processing},
  year      = {2008},
  volume    = {16},
  number    = {2},
  pages     = {372--381},
  month     = feb,
  doi       = {10.1109/tasl.2007.911644},
  groups    = {Explorative studies},
  publisher = {Institute of Electrical and Electronics Engineers ({IEEE})},

}

@Article{Korbar2018,
  author        = {Korbar, Bruno and Tran, Du and Torresani, Lorenzo},
  title         = {Cooperative Learning of Audio and Video Models from Self-supervised Synchronization},
  year          = {2018},
  month         = jun,
  abstract      = {There is a natural correlation between the visual and auditive elements of a video. In this work we leverage this connection to learn general and effective models for both audio and video analysis from self-supervised temporal synchronization. We demonstrate that a calibrated curriculum learning scheme, a careful choice of negative examples, and the use of a contrastive loss are critical ingredients to obtain powerful multi-sensory representations from models optimized to discern temporal synchronization of audio-video pairs. Without further finetuning, the resulting audio features achieve performance superior or comparable to the state-of-the-art on established audio classification benchmarks (DCASE2014 and ESC-50). At the same time, our visual subnet provides a very effective initialization to improve the accuracy of video-based action recognition models: compared to learning from scratch, our self-supervised pretraining yields a remarkable gain of +19.9% in action recognition accuracy on UCF101 and a boost of +17.7% on HMDB51.},
  archiveprefix = {arXiv},
  eprint        = {http://arxiv.org/abs/1807.00230v2},
  file          = {\:Korbar2018.pdf\:PDF:\:Korbar2018.pdf\:PDF:PDF;:Korbar2018.pdf:PDF},
  groups        = {PhD Milan},
  keywords      = {cs.CV},
  primaryclass  = {cs.CV},
}

@InProceedings{Meseguer-Brocal2018,
  author    = {Meseguer-Brocal, Gabriel and Cohen-Hadria, Alice and Geoffroy, Peeters},
  title     = {Dali: A Large Dataset of Synchronized Audio, Lyrics and Notes, Automatically Created Using Teacher-student Machine Learning Paradigm.},
  booktitle = {19th International Society for Music Information Retrieval Conference},
  year      = {2018},
  editor    = {ISMIR},
  month     = sep,
  file      = {\:Meseguer-Brocal2018.pdf\:PDF:\:Meseguer-Brocal2018.pdf\:PDF:PDF;:Meseguer-Brocal2018.pdf:PDF},
  groups    = {Resources},
}

@InCollection{Bent2001,
  author    = {Bent, Ian D. and Hughes, David W. and Provine, Robert C. and Rastall, Richard and Kilmer, Anne and Hiley, David and Szendrei, Janka and Payne, Thomas B. and Bent, Margaret and Chew, Geoffrey},
  title     = {Notation},
  year      = {2001},
  booktitle = {Grove Music Online},
  doi       = {10.1093/gmo/9781561592630.article.20114},
  groups    = {MMRP'19},
  publisher = {Oxford University Press},
}

@InCollection{Brown2001,
  author    = {Brown, Howard Mayer and Rosand, Ellen and Strohm, Reinhard and Noiray, Michel and Parker, Roger and Whittall, Arnold and Savage, Roger and Millington, Barry},
  title     = {Opera (i)},
  booktitle = {Grove Music Online},
  year      = {2001},
  date      = {2018-11-14},
  groups    = {MMRP'19},
  publisher = {Oxford University Press},
}

@InCollection{Mumma2003,
  author    = {Mumma, Gordon and Rye, Howard and Kernfeld, Barry and Sheridan, Chris},
  title     = {Recording},
  year      = {2003},
  booktitle = {Grove Music Online},
  date      = {2018-11-14},
  doi       = {10.1093/gmo/9781561592630.article.J371600},
  groups    = {MMRP'19},
  publisher = {Oxford University Press},
}

@InCollection{Kitahara2010,
  author    = {Kitahara, Tetsuro},
  title     = {Mid-level Representations of Musical Audio Signals for Music Information Retrieval},
  booktitle = {Advances in Music Information Retrieval},
  publisher = {Springer Berlin Heidelberg},
  year      = {2010},
  pages     = {65--91},
  doi       = {10.1007/978-3-642-11674-2_4},
  file      = {\:Kitahara2010.pdf\:PDF:\:Kitahara2010.pdf\:PDF:PDF;:Kitahara2010.pdf:PDF},
  groups    = {MMRP'19},
}

@InProceedings{Slizovskaia2017a,
  author    = {Slizovskaia, Olga and Gómez, Emilia and Haro, Gloria},
  title     = {Musical Instrument Recognition in User-generated Videos Using a Multimodal Convolutional Neural Network Architecture},
  booktitle = {Proceedings of the 2017 {ACM} on International Conference on Multimedia Retrieval - {ICMR} {\textquotesingle}17},
  year      = {2017},
  publisher = {{ACM} Press},
  doi       = {10.1145/3078971.3079002},
  file      = {\:Slizovskaia2017.pdf\:PDF:\:Slizovskaia2017.pdf\:PDF:PDF;:Slizovskaia2017.pdf:PDF},
  groups    = {Explorative studies},
  keywords  = {rank4},
}

@InProceedings{Slizovskaia2017b,
  author    = {Slizovskaia, Olga and Gómez, Emilia and Haro, Gloria},
  title     = {Musical Instrument Recognition in User-generated Videos Using a Multimodal Convolutional Neural Network Architecture},
  booktitle = {Proceedings of the 2017 {ACM} on International Conference on Multimedia Retrieval - {ICMR} {\textquotesingle}17},
  year      = {2017},
  publisher = {{ACM} Press},
  doi       = {10.1145/3078971.3079002},
  file      = {\:Slizovskaia2017.pdf\:PDF:\:Slizovskaia2017.pdf\:PDF:PDF;:Slizovskaia2017.pdf:PDF},
  groups    = {Explorative studies},
  keywords  = {rank4},
}

@InCollection{Pachet2005,
  author    = {Fran{\c{c}}ois Pachet},
  title     = {Musical Metadata and Knowledge Management},
  booktitle = {Encyclopedia of Knowledge Management, Second Edition},
  publisher = {{IGI} Global},
  year      = {2005},
  editor    = {David G. Schwartz and Dov Te'eni},
  pages     = {1192--1199},
  bibsource = {dblp computer science bibliography, https://dblp.org},

  file      = {\:Pachet2011.pdf\:PDF:\:Pachet2011.pdf\:PDF:PDF},
  groups    = {MMRP'19},

}

@Article{Minsky1991,
  author        = {Marvin Minsky},
  title         = {Logical Versus Analogical or Symbolic Versus Connectionist or Neat Versus Scruffy},
  journal       = {{AI} Magazine},
  year          = {1991},
  volume        = {12},
  number        = {2},
  pages         = {34--51},
  __markedentry = {[sapo:]},
  bibsource     = {dblp computer science bibliography, https://dblp.org},

  file          = {:Minsky1991.pdf:PDF},
  groups        = {MMRP'19},

}

@InProceedings{Benetos2012,
  author    = {Benetos, Emmanouil and Klapuri, Anssi and Dixon, Simon},
  title     = {{Score-informed transcription for automatic piano tutoring}},
  booktitle = {European Signal Processing Conference},
  year      = {2012},
  pages     = {2153--2157},
  abstract  = {In this paper, a score-informed transcription method for auto- matic piano tutoring is proposed. The method takes as input a recording made by a student which may contain mistakes, along with a reference score. The recording and the aligned synthesized score are automatically transcribed using the non-negative matrix factorization algorithm for multi-pitch estimation and hidden Markov models for note tracking. By comparing the two transcribed recordings, common errors occurring in transcription algorithms such as extra octave notes can be suppressed. The result is a piano-roll descrip- tion which shows the mistakes made by the student along with the correctly played notes. Evaluation was performed on six pieces recorded using a Disklavier piano, using both manually-aligned and automatically-aligned scores as an in- put. Results comparing the system output with ground-truth annotation of the original recording reach a weighted F- measure of 93{\%}, indicating that the proposed method can successfully analyze the student's performance.},
  file      = {:Benetos2012.pdf:PDF},
  groups    = {Piano tutoring},
  keywords  = {HMMs, Music signal analysis, NMF, score-informed transcription, rank3},
}

@InProceedings{Fukuda2015,
  author    = {Fukuda, Tsubasa and Ikemiya, Yukara and Itoyama, Katsutoshi and Yoshii, Kazuyoshi},
  title     = {A Score-informed Piano Tutoring System with Mistake Detection and Score Simplification},
  booktitle = {Proc of the Sound and Music Computing Conference (SMC)},
  year      = {2015},
  publisher = {Zenodo},
  doi       = {10.5281/zenodo.851129},
  file      = {:Fukuda2015.pdf:PDF},
  groups    = {Piano tutoring},
}

@Article{Wang2017,
  author       = {Wang, Siying and Ewert, Sebastian and Dixon, Simon},
  title        = {Identifying Missing and Extra Notes in Piano Recordings Using Score-informed Dictionary Learning},
  volume       = {25},
  number       = {10},
  pages        = {1877--1889},
  year         = {2017},
  doi          = {10.1109/taslp.2017.2724203},
  file         = {:Wang2017.pdf:PDF},
  groups       = {Piano tutoring},
  journal = {{IEEE}/{ACM} Transactions on Audio, Speech, and Language Processing},
  publisher    = {Institute of Electrical and Electronics Engineers ({IEEE})},
}

@InProceedings{Abeser2013,
  author    = {Abe{\ss}er, Jakob and Hasselhorn, Johannes and Dittmar, Christian and Lehmann, Andreas and Grollmisch, Sascha},
  title     = {Automatic quality assessment of vocal and instrumental performances of ninth-grade and tenth-grade pupils},
  booktitle = {International Symposium on Computer Music Multidisciplinary Research},
  year      = {2013},
  file      = {:Abeser2013.pdf:PDF},
  groups    = {Piano tutoring},
}

@InProceedings{Devaney2012,
  author    = {Johanna Devaney and Michael I. Mandel and Ichiro Fujinaga},
  title     = {A Study of Intonation in Three-Part Singing using the Automatic Music Performance Analysis and Comparison Toolkit {(AMPACT)}},
  booktitle = {Proceedings of the 13th International Society for Music Information Retrieval Conference, {ISMIR} 2012, Mosteiro S.Bento Da Vit{\'{o}}ria, Porto, Portugal, October 8-12, 2012},
  year      = {2012},
  editor    = {Fabien Gouyon and Perfecto Herrera and Luis Gustavo Martins and Meinard M{\"{u}}ller},
  pages     = {511--516},
  publisher = {{FEUP} Edi{\c{c}}{\~{o}}es},
  bibsource = {dblp computer science bibliography, https://dblp.org},

  groups    = {Piano tutoring},

}

@InProceedings{Mayor2009,
  author    = {Mayor, O and Bonada, J and Loscos, A},
  title     = {Performance Analysis and Scoring of the Singing Voice},
  booktitle = {AES 35th International Conference: Audio for Games},
  year      = {2009},
  groups    = {Piano tutoring},
}

@Article{Tsai2012,
  author        = {W. Tsai and H. Lee},
  title         = {Automatic Evaluation of Karaoke Singing Based on Pitch, Volume, and Rhythm Features},
  journal       = {and Language Processing IEEE Transactions on Audio, Speech},
  year          = {2012},
  volume        = {20},
  number        = {4},
  pages         = {1233--1243},
  month         = may,
  issn          = {1558-7916},
  __markedentry = {[sapo:]},
  doi           = {10.1109/TASL.2011.2174224},
  file          = {:Tsai2012.pdf:PDF},
  groups        = {Piano tutoring},
  keywords      = {audio signal processing, music, karaoke singing, pitch features, volume features, rhythm features, automatic singing evaluation system, karaoke scoring mechanism, karaoke video compact disk music, Pearson product-moment correlation coefficient, Rhythm, Humans, Accuracy, Timbre, Lead, Accompaniment, Karaoke, singing evaluation, solo vocal},
}

@Online{MIREX2016,
  author   = {{MIREX Community}},
  title    = {2016:Query by Singing/Humming},
  year     = {2016},
  groups   = {Query-by-humming},
  keywords = {rank3},
  url      = {https://www.music-ir.org/mirex/wiki/2016:Query\_by\_Singing/Humming},
}

@Article{Borjian2017,
  author       = {Borjian, Nastaran},
  title        = {A Survey on Query-by-example Based Music Information Retrieval},
  volume       = {158},
  number       = {8},
  year         = {2017},
  file         = {:Borjian2017.pdf:PDF},
  groups       = {Query-by-humming},
  journal      = {International Journal of Computer Applications},
  publisher    = {Foundation of Computer Science},
}

@Article{Bonnin2014,
  author     = {Bonnin, Geoffray and Jannach, Dietmar},
  title      = {Automated Generation of Music Playlists: Survey and Experiments},
  journal    = {ACM Comput. Surv.},
  year       = {2014},
  volume     = {47},
  number     = {2},
  pages      = {26:1--26:35},
  month      = nov,
  issn       = {0360-0300},
  acmid      = {2652481},
  address    = {New York, NY, USA},
  articleno  = {26},
  doi        = {10.1145/2652481},
  groups     = {recommendetion},
  issue_date = {January 2015},
  keywords   = {Music, algorithm, evaluation, playlist},
  numpages   = {35},
  publisher  = {ACM},
}

@Article{Deshmukh2018,
  author  = {Deshmukh, Puja and Kale, Geetanjali},
  title   = {{A Survey of Music Recommendation System}},
  journal = {International Journal of Scientific Research in Computer Science, Engineering and Information Technology (IJSRCSEIT)},
  year    = {2018},
  volume  = {3},
  number  = {3},
  pages   = {1721-1729},
  month   = mar,
  issn    = {2456-3307},
  file    = {:Deshmukh2018.pdf:PDF},
  groups  = {recommendetion},
}

@InProceedings{Itohara2011,
  author    = {T. Itohara and T. Otsuka and T. Mizumoto and T. Ogata and H. G. Okuno},
  title     = {Particle-filter based audio-visual beat-tracking for music robot ensemble with human guitarist},
  booktitle = {Proc. IEEE/RSJ Int. Conf. Intelligent Robots and Systems},
  year      = {2011},
  pages     = {118--124},
  month     = sep,
  abstract  = {This paper presents an audio-visual beat-tracking method for ensemble robots with a human guitarist. Beat-tracking, or estimation of tempo and beat times of music, is critical to the high quality of musical ensemble performance. Since a human plays the guitar in out-beat in back beat and syncopation, the main problems of beat-tracking of a human's guitar playing are twofold: tempo changes and varying note lengths. Most conventional methods have not addressed human's guitar playing. Therefore, they lack the adaptation of either of the problems. To solve the problems simultaneously, our method uses not only audio but visual features. We extract audio features with Spectro-Temporal Pattern Matching (STPM) and visual features with optical flow, mean shift and Hough transform. Our beat-tracking estimates tempo and beat time using a particle filter; both acoustic feature of guitar sounds and visual features of arm motions are represented as particles. The particle is determined based on prior distribution of audio and visual features, respectively Experimental results confirm that our integrated audio-visual approach is robust against tempo changes and varying note lengths. In addition, they also show that estimation convergence rate depends only a little on the number of particles. The real-time factor is 0.88 when the number of particles is 200, and this shows out method works in real-time.},
  doi       = {10.1109/IROS.2011.6094773},
  groups    = {Beat tracking},
  issn      = {2153-0866},
  keywords  = {acoustic signal processing, audio signal processing, audio-visual systems, feature extraction, Hough transforms, human-robot interaction, image matching, image sequences, music, musical acoustics, musical instruments, particle filtering (numerical methods), particle filter based audio-visual beat tracking, music robot ensemble, human guitarist, tempo estimation, beat times, back beat, syncopation, tempo changes, note length variation, visual features, audio feature extraction, spectrotemporal pattern matching, optical flow, mean shift, Hough transform, acoustic feature, guitar sounds, arm motions, integrated audio-visual approach, estimation convergence rate, Argon, Noise, Robots},
}

@PhdThesis{Berman2012,
  author = {Berman, David Ross},
  title  = {{AVISARME: Audio Visual Synchronization Algorithm for a Robotic Musician Ensemble}},
  school = {University of Maryland},
  year   = {2012},
  groups = {Beat tracking},
}

@InProceedings{Weinberg2009,
  author    = {Weinberg, Gil and Raman, Aparna and Mallikarjuna, Trishul},
  title     = {Interactive Jamming with Shimon: A Social Robotic Musician},
  booktitle = {Proceedings of the 4th ACM/IEEE International Conference on Human Robot Interaction},
  year      = {2009},
  series    = {HRI '09},
  pages     = {233--234},
  address   = {New York, NY, USA},
  publisher = {ACM},
  abstract  = {The paper introduces Shimon: a socially interactive and improvisational robotic marimba player. It presents the interaction schemes used by Shimon in the realization of an interactive musical jam session among human and robotic musicians.},
  acmid     = {1514152},
  doi       = {10.1145/1514095.1514152},
  groups    = {Beat tracking},
  isbn      = {978-1-60558-404-1},
  keywords  = {beat, haile, improvisation, interaction, jam, marimba, markov, melody, music, rhythm, robot, shimon, social},
  location  = {La Jolla, California, USA},
  numpages  = {2},
}

@InProceedings{Petersen2008,
  author        = {K. Petersen and J. Solis and A. Takanishi},
  title         = {Development of a real-time instrument tracking system for enabling the musical interaction with the Waseda Flutist Robot},
  booktitle     = {Proc. IEEE/RSJ Int. Conf. Intelligent Robots and Systems},
  year          = {2008},
  pages         = {313--318},
  month         = sep,
  __markedentry = {[sapo:]},
  abstract      = {The aim of this paper is to create an interface for human-robot interaction. Specifically, musical performance parameters (i.e. vibrato expression) of the Waseda Flutist Robot No.4 Refined IV (WF-4RIV) are to be manipulated. Our research is focused on enabling the WF-4RIV to interact with human players (musicians) in a natural way. In this paper, as a first approach, a vision processing algorithm, that is able to track the 3D-orientation and position of a musical instrument, was developed. In particular, the robot acquires image data through two cameras attached to its head. Using color histogram matching and a particle filter, the position of the musicianpsilas hands on the instrument are tracked. Analysis of this data determines orientation and location of the instrument. These parameters are mapped to manipulate the musical expression of the WF-4RIV, more specifically sound vibrato and volume values. We present preliminary experiments to determine if the robot may dynamically change musical parameters while interacting with a human player (i.e. vibrato etc.). From the experimental results, we may confirm the feasibility of the interaction during a performance, although further research must be carried out to consider the physical constraints of the flutist robot.},
  doi           = {10.1109/IROS.2008.4650831},
  groups        = {Beat tracking},
  issn          = {2153-0858},
  keywords      = {human-robot interaction, image colour analysis, image matching, intelligent robots, musical instruments, robot vision, real-time instrument tracking system, musical interaction, human-robot interaction, WF-4RIV, Waseda Flutist Robot No.4 Refined IV, vision processing algorithm, musical instrument, color histogram matching, particle filter, Robots, Instruments, Cameras, Humans, Image color analysis, Robot vision systems, Robot sensing systems},
}

@InProceedings{Lim2010,
  author        = {A. Lim and T. Mizumoto and L. Cahier and T. Otsuka and T. Takahashi and K. Komatani and T. Ogata and H. G. Okuno},
  title         = {Robot musical accompaniment: integrating audio and visual cues for real-time synchronization with a human flutist},
  booktitle     = {Proc. IEEE/RSJ Int. Conf. Intelligent Robots and Systems},
  year          = {2010},
  pages         = {1964--1969},
  month         = oct,
  __markedentry = {[sapo:]},
  abstract      = {Musicians often have the following problem: they have a music score that requires 2 or more players, but they have no one with whom to practice. So far, score-playing music robots exist, but they lack adaptive abilities to synchronize with fellow players' tempo variations. In other words, if the human speeds up their play, the robot should also increase its speed. However, computer accompaniment systems allow exactly this kind of adaptive ability. We present a first step towards giving these accompaniment abilities to a music robot. We introduce a new paradigm of beat tracking using 2 types of sensory input - visual and audio - using our own visual cue recognition system and state-of-the-art acoustic onset detection techniques. Preliminary experiments suggest that by coupling these two modalities, a robot accompanist can start and stop a performance in synchrony with a flutist, and detect tempo changes within half a second.},
  doi           = {10.1109/IROS.2010.5650427},
  groups        = {Beat tracking},
  issn          = {2153-0866},
  keywords      = {acoustic signal detection, humanoid robots, image recognition, music, musical instruments, robot vision, synchronisation, robot musical accompaniment, audio cue, real-time synchronization, human flutist, musician, score-playing music robot, computer accompaniment system, beat tracking, visual cue recognition system, acoustic onset detection, tempo change detection, Visualization, Humans, Robot kinematics, Synchronization, Robot sensing systems, Microphones},
}

@InProceedings{Hrybyk2010,
  author    = {Hrybyk, Alex},
  title     = {Combined audio and video analysis for guitar chord identification},
  booktitle = {11th International Society for Music Information Retrieval Conference (ISMIR 2010)},
  year      = {2010},
  groups    = {Spatial transcription},
}

@InProceedings{Paleari2008,
  author        = {M. Paleari and B. Huet and A. Schutz and D. Slock},
  title         = {A multimodal approach to music transcription},
  booktitle     = {Proc. 15th IEEE Int. Conf. Image Processing},
  year          = {2008},
  pages         = {93--96},
  month         = oct,
  abstract      = {Music transcription refers to extraction of a human readable and interpretable description from a recording of a music performance. Automatic music transcription remains, nowadays, a challenging research problem when dealing with polyphonic sounds or when removing certain constraints. Some instruments like guitars and violins add ambiguity to the problem as the same note can be played at different positions. When dealing with guitar music tablature are, often, preferred to the usual music score, as they present information in a more accessible way. Here, we address this issue with a system which uses the visual modality to support traditional audio transcription techniques. The system is composed of four modules which have been implemented and evaluated: a system which tracks the position of the fretboard on a video stream, a system which automatically detects the position of the guitar on the first fret to initialize the first system, a system which detects the position of the hand on the guitar, and finally a system which fuses the visual and audio information to extract a tablature. Results show that this kind of multimodal approach can easily disambiguate 89\% of notes in a deterministic way.},
  doi           = {10.1109/ICIP.2008.4711699},
  groups        = {Spatial transcription},
  issn          = {1522-4880},
  keywords      = {music, musical instruments, video signal processing, multimodal approach, music transcription, audio transcription, polyphonic sounds, guitar music tablature, music score, visual modality, video cameras, fretboard tracking, Data mining, Timbre, Fingers, Humans, Instruments, Information analysis, Multiple signal classification, Music, Streaming media, Fuses, Multimodal, Music Transcription, Guitar, Tablature},
}

@InProceedings{Gabrilovich2007,
  author    = {Gabrilovich, Evgeniy and Markovitch, Shaul},
  title     = {Computing semantic relatedness using wikipedia-based explicit semantic analysis.},
  booktitle = {IJcAI},
  year      = {2007},
  volume    = {7},
  pages     = {1606--1611},
  groups    = {PhD Milan},
}
